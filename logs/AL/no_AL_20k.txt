
loading file
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.15312628885128207
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.15125447172366316
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1486573496208627
Normal: h_loss: 0.0413608290758585 macro F 0.009166046456970504 micro F 0.0838033843674456 micro P 0.6332882273342354 micro R 0.0448705656759348
Multi only: h_loss: 0.07495349064917263 macro F 0.005173099880620771 micro F 0.016698779704560053
Jaccard: 0.05147502640712238
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.14180444600545428
Normal: h_loss: 0.0413527452628856 macro F 0.020172117039586917 micro F 0.25576489415872555 micro P 0.5299969852276153 micro R 0.16855225311601152
Multi only: h_loss: 0.07664251444237737 macro F 0.01596638655462185 micro F 0.08850072780203785
Jaccard: 0.1902821789648408
saving best model ...
Training Loss for epoch 1: 0.1606113401889801
Evaluating:
Evaluation results:
Evaluation Loss 0.14117738450069997
Normal: h_loss: 0.04113448231261722 macro F 0.020141707528799006 micro F 0.25229593710969067 micro P 0.5397673687519647 micro R 0.16462128475551294
Multi only: h_loss: 0.07629981396259669 macro F 0.01606600086843248 micro F 0.08672721945502491
Jaccard: 0.18588727931190582
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.04040643344125931 macro F 0.02042183432858674 micro F 0.2579163645153493
Multi only: h_loss: 0.07620754394947943 macro F 0.015357142857142857 micro F 0.08784473953013278
Jaccard: 0.18859406670351947
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.13343223971902865
Normal: h_loss: 0.03877805083101597 macro F 0.05281643667279977 micro F 0.31725021349274124 micro P 0.6154058531198233 micro R 0.2137104506232023
Multi only: h_loss: 0.0724077156565162 macro F 0.05071535639554037 micro F 0.16722972972972971
Jaccard: 0.23426889995473066
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.12931893279564333
Normal: h_loss: 0.03850724309642372 macro F 0.06994471085738994 micro F 0.3229336934119821 micro P 0.6240043943971436 micro R 0.21783317353787152
Multi only: h_loss: 0.07130617840007833 macro F 0.06494089713267795 micro F 0.18059071729957807
Jaccard: 0.22851591972234792
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.12472975399765077
Normal: h_loss: 0.037516976007243096 macro F 0.09494950157411987 micro F 0.2787878787878788 micro P 0.7352459016393442 micro R 0.1720038350910834
Multi only: h_loss: 0.06856457456183296 macro F 0.0847137287168496 micro F 0.1864652918966018
Jaccard: 0.18370869171570842
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.11751942701162175
Normal: h_loss: 0.03508779020888573 macro F 0.12027541111451089 micro F 0.39204426080257726 micro P 0.7272018706157444 micro R 0.26836049856184085
Multi only: h_loss: 0.06667972192303927 macro F 0.10798799292689405 micro F 0.25776566757493186
Jaccard: 0.28816772295156173
saving best model ...
Training Loss for epoch 1: 0.12990958545207978
Evaluating:
Evaluation results:
Evaluation Loss 0.1188263507444376
Normal: h_loss: 0.03601742870076958 macro F 0.11418788486328221 micro F 0.31794871794871793 micro P 0.788235294117647 micro R 0.199137104506232
Multi only: h_loss: 0.06601879956917654 macro F 0.10437538663602368 micro F 0.2383507483761649
Jaccard: 0.2094367738041345
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03560241122430177 macro F 0.11405698345241166 micro F 0.31553643724696356
Multi only: h_loss: 0.06630824372759857 macro F 0.09983817545313056 micro F 0.2260956175298805
Jaccard: 0.20715865118850194
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.11361615697740698
Normal: h_loss: 0.034905904416995405 macro F 0.12457400931021778 micro F 0.3830547221031576 micro P 0.7514013452914798 micro R 0.2570469798657718
Multi only: h_loss: 0.06611671399197101 macro F 0.10937759869327288 micro F 0.2589849108367627
Jaccard: 0.27417194809114226
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10931106782933607
Normal: h_loss: 0.03480081484834767 macro F 0.13938780922448316 micro F 0.4319081551860649 micro P 0.692551840880237 micro R 0.3138063279002876
Multi only: h_loss: 0.06574953490649173 macro F 0.12693274652835032 micro F 0.2863974495217853
Jaccard: 0.3360212011468236
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1076598218855085
Normal: h_loss: 0.03436833085429736 macro F 0.14174754511193466 micro F 0.4490377761938703 micro P 0.6925844493304018 micro R 0.33221476510067116
Multi only: h_loss: 0.06543131303240968 macro F 0.1341214667148257 micro F 0.3051728619703665
Jaccard: 0.3534781952618078
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10427345659612476
Normal: h_loss: 0.03419452887537994 macro F 0.14850091123122075 micro F 0.45029239766081874 micro P 0.6985887096774194 micro R 0.33221476510067116
Multi only: h_loss: 0.06511309115832763 macro F 0.1410189327755002 micro F 0.30656934306569344
Jaccard: 0.35375169760072445
saving best model ...
Training Loss for epoch 1: 0.10978679832220077
Evaluating:
Evaluation results:
Evaluation Loss 0.10444530864925393
Normal: h_loss: 0.034146025997542524 macro F 0.14439883646705853 micro F 0.44384463462804474 micro P 0.7081932773109244 micro R 0.32320230105465003
Multi only: h_loss: 0.06592088514638206 macro F 0.13102517788157927 micro F 0.28737761312516535
Jaccard: 0.34704617473970123
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03374661086103872 macro F 0.13830955325390695 micro F 0.44417949273791457
Multi only: h_loss: 0.06609489674005803 macro F 0.11827560127883555 micro F 0.2858460119870908
Jaccard: 0.3460936060438547
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10291875178064357
Normal: h_loss: 0.03410964883916446 macro F 0.20418296604337247 micro F 0.4780107626646873 micro P 0.6735227470803555 micro R 0.3704697986577181
Multi only: h_loss: 0.06601879956917654 macro F 0.17833162812422995 micro F 0.3160030433679939
Jaccard: 0.3969556360344048
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10056317060673167
Normal: h_loss: 0.03375396106835672 macro F 0.20685105998760697 micro F 0.44507940726958595 micro P 0.7250487118423902 micro R 0.32109300095877275
Multi only: h_loss: 0.061808479389014 macro F 0.1900229013436747 micro F 0.3553740107224917
Jaccard: 0.33084351893767927
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0978534184177033
Normal: h_loss: 0.033159800814848345 macro F 0.21631391946744727 micro F 0.4871858982372797 micro P 0.6998922413793104 micro R 0.37363374880153405
Multi only: h_loss: 0.0626897091941643 macro F 0.19670603471511458 micro F 0.35539894286433427
Jaccard: 0.39496944318696264
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09708431693980359
Normal: h_loss: 0.032783903511608356 macro F 0.23448796988342127 micro F 0.4685145141209619 micro P 0.7400124197888636 micro R 0.34276126558005754
Multi only: h_loss: 0.06131890727504161 macro F 0.2068773120377895 micro F 0.3594988493991307
Jaccard: 0.3571412403802626
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.09733260153532028
Evaluating:
Evaluation results:
Evaluation Loss 0.09757417112912674
Normal: h_loss: 0.03315171700187544 macro F 0.24671761065983902 micro F 0.4767797907629497 micro P 0.7123522683949676 micro R 0.3582933844678811
Multi only: h_loss: 0.06031528444139822 macro F 0.2259299612228189 micro F 0.38430784607696145
Jaccard: 0.36709106684774423
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.032845034088815186 macro F 0.24328041978167594 micro F 0.47380073800738004
Multi only: h_loss: 0.05901177675371224 macro F 0.22588369480328382 micro F 0.398434101783384
Jaccard: 0.3590074319759228
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09982574121303935
Normal: h_loss: 0.03368120675160059 macro F 0.23659964870637856 micro F 0.47993509330337647 micro P 0.687466475952083 micro R 0.36864813039309685
Multi only: h_loss: 0.06149025751493195 macro F 0.22168029957762067 micro F 0.38461538461538464
Jaccard: 0.37785196921684056
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09864282487284927
Normal: h_loss: 0.033446776175386404 macro F 0.2449931352011438 micro F 0.4626274433404767 micro P 0.7168444355001006 micro R 0.34151486097794825
Multi only: h_loss: 0.06173504357191814 macro F 0.21941445429878947 micro F 0.3582697201017812
Jaccard: 0.355943488758111
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09760000497008298
Normal: h_loss: 0.03324872275755028 macro F 0.262564017823639 micro F 0.482315921963499 micro P 0.7018315018315018 micro R 0.3674017257909875
Multi only: h_loss: 0.06070694213257613 macro F 0.24201894490896536 micro F 0.38916256157635465
Jaccard: 0.3765674513354461
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09504280371558564
Normal: h_loss: 0.03296174739701222 macro F 0.24704929502100467 micro F 0.49996934208105953 micro P 0.6934852866133696 micro R 0.39089165867689357
Multi only: h_loss: 0.06178400078331538 macro F 0.22555465796583965 micro F 0.3831867057673509
Jaccard: 0.4058397464916254
saving best model ...
Training Loss for epoch 1: 0.09497441855669021
Evaluating:
Evaluation results:
Evaluation Loss 0.09505753544268429
Normal: h_loss: 0.033095130311065125 macro F 0.24363859617511668 micro F 0.4969898021870009 micro P 0.6916894664842681 micro R 0.387823585810163
Multi only: h_loss: 0.062249094291589153 macro F 0.21859899002000008 micro F 0.37256353318529484
Jaccard: 0.4042817262713146
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.032779225565295216 macro F 0.24275531521150667 micro F 0.49528827642111667
Multi only: h_loss: 0.06178528759173921 macro F 0.2116851060909151 micro F 0.37261698440207974
Jaccard: 0.3998218782630061
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09553279904096361
Normal: h_loss: 0.03296578930349867 macro F 0.30272959513224984 micro F 0.5223146304322361 micro P 0.6711318482841662 micro R 0.42751677852348996
Multi only: h_loss: 0.05977675511602859 macro F 0.2651908380276736 micro F 0.41967680608365016
Jaccard: 0.434585785423269
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09376414018000559
Normal: h_loss: 0.032331209985125786 macro F 0.2846837165555804 micro F 0.4895667155893051 micro P 0.7319213890478916 micro R 0.36778523489932885
Multi only: h_loss: 0.0601684128072065 macro F 0.24081227945279027 micro F 0.38147961751383996
Jaccard: 0.3801625924249285
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09370353472730648
Normal: h_loss: 0.03236758714350385 macro F 0.277724664933435 micro F 0.5024850894632207 micro P 0.7137310271796682 micro R 0.38772770853307764
Multi only: h_loss: 0.06119651424654852 macro F 0.2411468560921663 micro F 0.3781094527363184
Jaccard: 0.40418930134299125
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09376986353793475
Normal: h_loss: 0.032068486063506436 macro F 0.28836886261482414 micro F 0.5185679611650486 micro P 0.7062809917355372 micro R 0.4096836049856184
Multi only: h_loss: 0.06109859982375404 macro F 0.24768640643515155 micro F 0.3849186791522918
Jaccard: 0.4293458578542329
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.08834057037830353
Evaluating:
Evaluation results:
Evaluation Loss 0.09480394808859671
Normal: h_loss: 0.03255351484188062 macro F 0.28491085502774754 micro F 0.5151697568023117 micro P 0.6921708185053381 micro R 0.4102588686481304
Multi only: h_loss: 0.06229805150298639 macro F 0.24048742031837614 micro F 0.3708281829419036
Jaccard: 0.43152444545043034
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03252915317591935 macro F 0.2823604088547894 micro F 0.5105455985741163
Multi only: h_loss: 0.06225465096432838 macro F 0.2348111304249982 micro F 0.3772940674349125
Jaccard: 0.42282415085068475
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09618321416630579
Normal: h_loss: 0.032395880488909005 macro F 0.2668805027813133 micro F 0.5149773071104388 micro P 0.6981132075471698 micro R 0.40795781399808245
Multi only: h_loss: 0.06134338588074023 macro F 0.23539019899976782 micro F 0.38878048780487806
Jaccard: 0.42744643126603327
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09479377100698654
Normal: h_loss: 0.03276773588566255 macro F 0.31326783739460723 micro F 0.5377729631107817 micro P 0.6633844422562949 micro R 0.45215723873441993
Multi only: h_loss: 0.059189268579261725 macro F 0.2845599786946386 micro F 0.439239332096475
Jaccard: 0.456958276746643
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09258195976593905
Normal: h_loss: 0.03216144991269482 macro F 0.28123854807497684 micro F 0.5225895482090357 micro P 0.6982523649190315 micro R 0.4175455417066155
Multi only: h_loss: 0.0604131988641927 macro F 0.24063551630676616 micro F 0.3992210321324245
Jaccard: 0.4344631809265131
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09228947192882456
Normal: h_loss: 0.03217357563215417 macro F 0.3027725407627497 micro F 0.5205974463984583 micro P 0.700032393909945 micro R 0.41438159156279963
Multi only: h_loss: 0.059336140213453445 macro F 0.2619888429418596 micro F 0.4133591481122943
Jaccard: 0.4276256224535989
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.08704862920045853
Evaluating:
Evaluation results:
Evaluation Loss 0.09294871383019031
Normal: h_loss: 0.03228674901377482 macro F 0.3128825201884836 micro F 0.5249197097656715 micro P 0.6912593984962406 micro R 0.42310642377756474
Multi only: h_loss: 0.05869969646528934 macro F 0.2746601999451139 micro F 0.4254911356013416
Jaccard: 0.43390674513354477
patience 3 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03190397220247967 macro F 0.3134719863742164 micro F 0.5247058823529412
Multi only: h_loss: 0.059353131933777094 macro F 0.2554982404297928 micro F 0.42017507294706125
Jaccard: 0.4300565075855292
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09468912033574135
Normal: h_loss: 0.0325050119640432 macro F 0.3122100314258668 micro F 0.5275525790153918 micro P 0.681128640776699 micro R 0.4304889741131352
Multi only: h_loss: 0.05943405463624792 macro F 0.2799973224251681 micro F 0.4254614292475154
Jaccard: 0.44041232835370486
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0930982205313544
Normal: h_loss: 0.03257776628079933 macro F 0.2910845962500615 micro F 0.5122836742103352 micro P 0.6943897637795275 micro R 0.40584851390220517
Multi only: h_loss: 0.06070694213257613 macro F 0.24904516817407085 micro F 0.39453125
Jaccard: 0.4223951259996984
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09398627281357606
Normal: h_loss: 0.03238375476944966 macro F 0.31076873986969206 micro F 0.5308034668540642 micro P 0.681913933192898 micro R 0.43451581975071907
Multi only: h_loss: 0.05987466953882307 macro F 0.26479689381482263 micro F 0.41706387035271686
Jaccard: 0.4489305115436854
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09244462552249783
Normal: h_loss: 0.032351419517558044 macro F 0.3417528601114409 micro F 0.5145560407569141 micro P 0.7002310993727303 micro R 0.40671140939597317
Multi only: h_loss: 0.05943405463624792 macro F 0.3005093508050587 micro F 0.4103933948518698
Jaccard: 0.4191791157386452
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.08434276220798492
Evaluating:
Evaluation results:
Evaluation Loss 0.09245486384626156
Normal: h_loss: 0.0322463299489103 macro F 0.34096420359433605 micro F 0.5100110551529297 micro P 0.709501025290499 micro R 0.3980824544582934
Multi only: h_loss: 0.059580926270439635 macro F 0.2989713586177071 micro F 0.40372366487016176
Jaccard: 0.41083257884412283
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03191055305483166 macro F 0.3438748040720324 micro F 0.5086634917418178
Multi only: h_loss: 0.059481140126301416 macro F 0.2760276589896943 micro F 0.39757994814174585
Jaccard: 0.40895829494502783
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09500873932381354
Normal: h_loss: 0.03269902347539287 macro F 0.31526944838313514 micro F 0.5387685290763968 micro P 0.6645569620253164 micro R 0.45302013422818793
Multi only: h_loss: 0.059972583961617545 macro F 0.2826890312561946 micro F 0.43208159480760316
Jaccard: 0.46817941753432973
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09390622700175423
Normal: h_loss: 0.03252926340296191 macro F 0.3530643707853697 micro F 0.5262538262302803 micro P 0.6816102470265325 micro R 0.42857142857142855
Multi only: h_loss: 0.05796533829433075 macro F 0.3133935509622338 micro F 0.44801864801864805
Jaccard: 0.43271653840350144
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09392938728163068
Normal: h_loss: 0.03240800620836836 macro F 0.3545691993626619 micro F 0.5276861451460886 micro P 0.6842346471127406 micro R 0.42943432406519655
Multi only: h_loss: 0.05830803877411143 macro F 0.3158915015649308 micro F 0.43714555765595464
Jaccard: 0.43622114078768726
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09132490505415047
Normal: h_loss: 0.03170875638621225 macro F 0.3166744098538158 micro F 0.5268681020445087 micro P 0.7101284344009104 micro R 0.41879194630872485
Multi only: h_loss: 0.05977675511602859 macro F 0.2687505367336073 micro F 0.40670553935860054
Jaccard: 0.43660404406217024
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.07961078441143035
Evaluating:
Evaluation results:
Evaluation Loss 0.09077401705264991
Normal: h_loss: 0.0317127982926987 macro F 0.3342862673518594 micro F 0.5386876763875823 micro P 0.6964122833688051 micro R 0.4392138063279003
Multi only: h_loss: 0.05945853324194654 macro F 0.28417408327145205 micro F 0.42014800668417285
Jaccard: 0.45678663045118484
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031140593329648057 macro F 0.341507307501228 micro F 0.542713567839196
Multi only: h_loss: 0.05824372759856631 macro F 0.2786646300724446 micro F 0.4300626304801671
Jaccard: 0.457158651188502
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09567059552395651
Normal: h_loss: 0.03306279505917351 macro F 0.3439069766290528 micro F 0.5482160609742627 micro P 0.6465607087024492 micro R 0.47583892617449663
Multi only: h_loss: 0.059972583961617545 macro F 0.30858200412091585 micro F 0.4383310408069692
Jaccard: 0.4814244756299989
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09282119329993463
Normal: h_loss: 0.0326303110651232 macro F 0.33220084167950475 micro F 0.5481614148989757 micro P 0.6584644345838375 micro R 0.46951102588686483
Multi only: h_loss: 0.05798981690002938 macro F 0.2894006772093795 micro F 0.4547756041426928
Jaccard: 0.4762807454353405
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0949745369597008
Normal: h_loss: 0.032614143439177394 macro F 0.33120146663520605 micro F 0.5504484929522536 micro P 0.6570022609389546 micro R 0.473633748801534
Multi only: h_loss: 0.059972583961617545 macro F 0.28881014175515923 micro F 0.4388456252863033
Jaccard: 0.48679643881092516
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09200671501036088
Normal: h_loss: 0.03276773588566255 macro F 0.3638692582517236 micro F 0.5214568207307715 micro P 0.6783904162187068 micro R 0.42348993288590603
Multi only: h_loss: 0.05791638108293352 macro F 0.31304782741503695 micro F 0.4393364928909953
Jaccard: 0.4290534932850465
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.07785472192764283
Evaluating:
Evaluation results:
Evaluation Loss 0.09270215270077012
Normal: h_loss: 0.032949621677552866 macro F 0.37149225312046846 micro F 0.5233863423760523 micro P 0.6706622715013485 micro R 0.42914669223394053
Multi only: h_loss: 0.056619014980906684 macro F 0.3295452308659435 micro F 0.4604618614415676
Jaccard: 0.42966651576882514
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03206849351127958 macro F 0.37687625988443896 micro F 0.5337288297770548
Multi only: h_loss: 0.05611025772316095 macro F 0.3281101933136639 micro F 0.4665314401622718
Jaccard: 0.43896259443523133
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09386155562076487
Normal: h_loss: 0.032707107288365775 macro F 0.36228831697246877 micro F 0.5379168570123345 micro P 0.6650663654334934 micro R 0.45158197507190795
Multi only: h_loss: 0.05867521785959072 macro F 0.31726524769404696 micro F 0.44603651490640167
Jaccard: 0.45915761279613754
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09632676912850252
Normal: h_loss: 0.03358824290241221 macro F 0.3599662485599407 micro F 0.5569417786308382 micro P 0.6273120345904396 micro R 0.5007670182166827
Multi only: h_loss: 0.05862626064819348 macro F 0.3200981589788259 micro F 0.469545957918051
Jaccard: 0.5000848800362157
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09479053380897866
Normal: h_loss: 0.032880909267283195 macro F 0.36155949969385875 micro F 0.5465187580132672 micro P 0.6528166200559329 micro R 0.4699904122722915
Multi only: h_loss: 0.05767159502594732 macro F 0.31771662788051147 micro F 0.4657596371882085
Jaccard: 0.47399275690357656
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09365607318989072
Normal: h_loss: 0.032302916639720625 macro F 0.3871818205614867 micro F 0.5578667846868777 micro P 0.6594297671985352 micro R 0.4834132310642378
Multi only: h_loss: 0.05803877411142661 macro F 0.34721148784363026 micro F 0.46393850327831787
Jaccard: 0.49032178964840845
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.07253888940811157
Evaluating:
Evaluation results:
Evaluation Loss 0.09363167279139802
Normal: h_loss: 0.03242821574080062 macro F 0.3928359277117376 micro F 0.5520629780581765 micro P 0.6608742146771822 micro R 0.47401725790987537
Multi only: h_loss: 0.05796533829433075 macro F 0.3542284032430655 micro F 0.46108329540282206
Jaccard: 0.4808340878225444
patience 3 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03279896812235121 macro F 0.36574347953191355 micro F 0.5620386643233743
Multi only: h_loss: 0.058286396996074415 macro F 0.3053679804021603 micro F 0.46930846930846937
Jaccard: 0.49996928935569096
