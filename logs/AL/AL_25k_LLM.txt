
loading file
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.15312628885128207
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.15125447172366316
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1486573496208627
Normal: h_loss: 0.0413608290758585 macro F 0.009166046456970504 micro F 0.0838033843674456 micro P 0.6332882273342354 micro R 0.0448705656759348
Multi only: h_loss: 0.07495349064917263 macro F 0.005173099880620771 micro F 0.016698779704560053
Jaccard: 0.05147502640712238
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.14180444600545428
Normal: h_loss: 0.0413527452628856 macro F 0.020172117039586917 micro F 0.25576489415872555 micro P 0.5299969852276153 micro R 0.16855225311601152
Multi only: h_loss: 0.07664251444237737 macro F 0.01596638655462185 micro F 0.08850072780203785
Jaccard: 0.1902821789648408
saving best model ...
Training Loss for epoch 1: 0.1606113401889801
Evaluating:
Evaluation results:
Evaluation Loss 0.14117738450069997
Normal: h_loss: 0.04113448231261722 macro F 0.020141707528799006 micro F 0.25229593710969067 micro P 0.5397673687519647 micro R 0.16462128475551294
Multi only: h_loss: 0.07629981396259669 macro F 0.01606600086843248 micro F 0.08672721945502491
Jaccard: 0.18588727931190582
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.04040643344125931 macro F 0.02042183432858674 micro F 0.2579163645153493
Multi only: h_loss: 0.07620754394947943 macro F 0.015357142857142857 micro F 0.08784473953013278
Jaccard: 0.18859406670351947
Evaluating:
20500
20500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.13252755627204735
Normal: h_loss: 0.03931966630020048 macro F 0.04156722101626614 micro F 0.1674084217733653 micro P 0.7799043062200957 micro R 0.0937679769894535
Multi only: h_loss: 0.07140409282287281 macro F 0.038627572241017614 micro F 0.10273761919409412
Jaccard: 0.10094122529047836
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.12891000632033084
Normal: h_loss: 0.039020565220203064 macro F 0.06590073499127067 micro F 0.2289137380191693 micro P 0.6856459330143541 micro R 0.137392138063279
Multi only: h_loss: 0.07003329090375013 macro F 0.06233704856782036 micro F 0.15028215028215028
Jaccard: 0.14487135958955785
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.12194635093401006
Normal: h_loss: 0.03625590118347022 macro F 0.09892191001346748 micro F 0.38120860927152317 micro P 0.6795376291195278 micro R 0.26490891658676896
Multi only: h_loss: 0.06917653970429845 macro F 0.08801600133201586 micro F 0.22786885245901642
Jaccard: 0.28734721593481205
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.11723650592573631
Normal: h_loss: 0.035985093448877965 macro F 0.12558173317656002 micro F 0.3569519682195738 micro P 0.7235724743777452 micro R 0.23691275167785236
Multi only: h_loss: 0.06658180750024478 macro F 0.11142599227520551 micro F 0.2452830188679245
Jaccard: 0.2525275388561943
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.13091826510429383
Evaluating:
Evaluation results:
Evaluation Loss 0.11865511797317935
Normal: h_loss: 0.03628419452887538 macro F 0.12600788316342595 micro F 0.3778501628664495 micro P 0.6816704176044011 micro R 0.2613614573346117
Multi only: h_loss: 0.06626358562616273 macro F 0.1177486304566545 micro F 0.2697599136768276
Jaccard: 0.2771804738192243
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03536550053962989 macro F 0.1254793354388957 micro F 0.3863895866636218
Multi only: h_loss: 0.06460146782727427 macro F 0.12021782363417702 micro F 0.29119850187265917
Jaccard: 0.27915975677169697
Evaluating:
21000
21000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.11761968055020743
Normal: h_loss: 0.03585979434779797 macro F 0.1338850610058782 micro F 0.4115930494760579 micro P 0.6675989672977625 micro R 0.2975071907957814
Multi only: h_loss: 0.06606775678057378 macro F 0.12459615772629018 micro F 0.2914150695720662
Jaccard: 0.3151784366983552
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.11312503473231997
Normal: h_loss: 0.035099915928345084 macro F 0.13965870860210622 micro F 0.3917915674464211 micro P 0.7268711018711018 micro R 0.26816874400767016
Multi only: h_loss: 0.06479486928424556 macro F 0.12701572312381598 micro F 0.2844011895106785
Jaccard: 0.28323524973592873
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10907718021508632
Normal: h_loss: 0.034263241285649615 macro F 0.12896363338424682 micro F 0.4284269435641562 micro P 0.721881390593047 micro R 0.3046021093000959
Multi only: h_loss: 0.06567609908939587 macro F 0.1180864785821579 micro F 0.2843424913310216
Jaccard: 0.32620341029123284
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10826515523070067
Normal: h_loss: 0.03455830045916058 macro F 0.13291479698606892 micro F 0.43151595744680854 micro P 0.7039045553145337 micro R 0.31112176414189835
Multi only: h_loss: 0.06366885342210908 macro F 0.130829933245503 micro F 0.32564169043297897
Jaccard: 0.3236947336653086
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.1175173769621622
Evaluating:
Evaluation results:
Evaluation Loss 0.1119856133065433
Normal: h_loss: 0.03599721916833732 macro F 0.1320773607560878 micro F 0.46271718146718144 micro P 0.6239830784249919 micro R 0.36768935762224353
Multi only: h_loss: 0.06491726231273867 macro F 0.13254408756043726 micro F 0.3509544787077827
Jaccard: 0.380045646597254
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03565505804311774 macro F 0.13593075952369071 micro F 0.4617524339360223
Multi only: h_loss: 0.06481481481481481 macro F 0.1337661164449701 micro F 0.35168587281263336
Jaccard: 0.3774184632393587
Evaluating:
21500
21500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10384625290221572
Normal: h_loss: 0.03385096682403156 macro F 0.15775901658295924 micro F 0.42891237640640983 micro P 0.7426210153482881 micro R 0.3015340364333653
Multi only: h_loss: 0.06381572505630079 macro F 0.1492598304394397 micro F 0.3083045900769435
Jaccard: 0.3185642070318394
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10351897453347068
Normal: h_loss: 0.03372162581646511 macro F 0.18522272077643523 micro F 0.4884419645594457 micro P 0.6774961728185066 micro R 0.3818791946308725
Multi only: h_loss: 0.06295897385684911 macro F 0.17662651517475175 micro F 0.3705335291238375
Jaccard: 0.3995114682360045
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09875712515394199
Normal: h_loss: 0.033572075276466405 macro F 0.20464534418407787 micro F 0.48804240631163714 micro P 0.6832930617880566 micro R 0.37957813998082457
Multi only: h_loss: 0.06320375991383531 macro F 0.18547103750684815 micro F 0.35417708854427216
Jaccard: 0.39938886373924876
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09855694918745295
Normal: h_loss: 0.03331743516781996 macro F 0.2400805640456976 micro F 0.4338987706888263 micro P 0.7647058823529411 micro R 0.3028763183125599
Multi only: h_loss: 0.06131890727504161 macro F 0.20927098799908797 micro F 0.345782188560982
Jaccard: 0.3142730496453901
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.1048357916848604
Evaluating:
Evaluation results:
Evaluation Loss 0.09814663776422097
Normal: h_loss: 0.033119381749983835 macro F 0.22142668073853675 micro F 0.5041752390173061 micro P 0.6833989501312336 micro R 0.39942473633748804
Multi only: h_loss: 0.06295897385684911 macro F 0.19435282534001747 micro F 0.36273538156590684
Jaccard: 0.42427757658065507
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.032983231988207115 macro F 0.21897689942362883 micro F 0.4973926995587646
Multi only: h_loss: 0.06490015360983103 macro F 0.16795355597083955 micro F 0.33435448577680527
Jaccard: 0.4158221239481604
Evaluating:
22000
22000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09829225927092144
Normal: h_loss: 0.03324063894457738 macro F 0.2559275919162169 micro F 0.5132575757575757 micro P 0.6705845963501392 micro R 0.41572387344199424
Multi only: h_loss: 0.0592871830020562 macro F 0.24043542644390795 micro F 0.4219570405727924
Jaccard: 0.42152746340727393
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09631801486662903
Normal: h_loss: 0.032440341460259974 macro F 0.2703675602952191 micro F 0.49163921966050167 micro P 0.7243374393430384 micro R 0.37209971236816874
Multi only: h_loss: 0.05967884069323411 macro F 0.23376537191453164 micro F 0.38682092555331987
Jaccard: 0.38482722197072605
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0945691630048631
Normal: h_loss: 0.03255351484188062 macro F 0.31636385683156265 micro F 0.5226410621147463 micro P 0.6844147780192487 micro R 0.4227229146692234
Multi only: h_loss: 0.05906687555076863 macro F 0.2820785171261145 micro F 0.4209263258939285
Jaccard: 0.4344990191640262
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09406953387977636
Normal: h_loss: 0.03203615081161482 macro F 0.26286028125787053 micro F 0.5028850978424485 micro P 0.7270583968081248 micro R 0.38437200383509107
Multi only: h_loss: 0.061147557035151276 macro F 0.23032461980216876 micro F 0.3726770467101959
Jaccard: 0.4041893013429911
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.09795326439358971
Evaluating:
Evaluation results:
Evaluation Loss 0.09332984708495785
Normal: h_loss: 0.031914893617021274 macro F 0.3089250694489306 micro F 0.5231308129001087 micro P 0.7067558746736292 micro R 0.4152444870565676
Multi only: h_loss: 0.05855282483109762 macro F 0.27395741480262364 micro F 0.42361445783132523
Jaccard: 0.42520371208691743
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031548606175471845 macro F 0.3091083448334862 micro F 0.5216523647974457
Multi only: h_loss: 0.058713090971155485 macro F 0.24763263353820347 micro F 0.4114627887082977
Jaccard: 0.42075118235980596
Evaluating:
22500
22500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09410470845864984
Normal: h_loss: 0.03220995279053224 macro F 0.2858601803771923 micro F 0.5151201703681167 micro P 0.7049125728559533 micro R 0.40584851390220517
Multi only: h_loss: 0.0592871830020562 macro F 0.24563075622606675 micro F 0.40926829268292686
Jaccard: 0.4190187867813493
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09385469118398798
Normal: h_loss: 0.03253330530944836 macro F 0.3278591979625966 micro F 0.5484938576316823 micro P 0.6609436257942409 micro R 0.46874400767018215
Multi only: h_loss: 0.059189268579261725 macro F 0.2893470325824225 micro F 0.44131238447319776
Jaccard: 0.4770163724158747
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09197246721423336
Normal: h_loss: 0.03206040225053353 macro F 0.34214091678755515 micro F 0.5331371394938199 micro P 0.6903963414634147 micro R 0.4342281879194631
Multi only: h_loss: 0.057696073631645944 macro F 0.3070704107561815 micro F 0.44371017229171583
Jaccard: 0.44154028972385734
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09282893797778177
Normal: h_loss: 0.03280007113755416 macro F 0.2894439933089785 micro F 0.5381068928225853 micro P 0.6621375542793109 micro R 0.4532118887823586
Multi only: h_loss: 0.061220992852247136 macro F 0.24333734394877218 micro F 0.4077669902912622
Jaccard: 0.4694526180775619
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.09453657122585508
Evaluating:
Evaluation results:
Evaluation Loss 0.0914774480827689
Normal: h_loss: 0.03247267671215159 macro F 0.33267978164981743 micro F 0.5194401244167963 micro P 0.6905216284987278 micro R 0.4162991371045062
Multi only: h_loss: 0.05806325271712523 macro F 0.29269290974195183 micro F 0.4325358851674641
Jaccard: 0.4249660479855142
patience 3 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031693384927215774 macro F 0.3475575003987201 micro F 0.5242986961675227
Multi only: h_loss: 0.057646356033452806 macro F 0.2683029616596235 micro F 0.432114333753678
Jaccard: 0.42640194091272043
Evaluating:
23000
23000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09506680829228724
Normal: h_loss: 0.03288899308025609 macro F 0.32005332976184514 micro F 0.4945648798061991 micro P 0.7022402540130535 micro R 0.3816874400767018
Multi only: h_loss: 0.058479389014001765 macro F 0.27761289153247853 micro F 0.4146042636608674
Jaccard: 0.3874434133091901
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09238346957172835
Normal: h_loss: 0.032133156567289656 macro F 0.32595267641274844 micro F 0.5254864509967769 micro P 0.696078431372549 micro R 0.4220517737296261
Multi only: h_loss: 0.05967884069323411 macro F 0.2765448847297377 micro F 0.41196333815726
Jaccard: 0.4364569186660634
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09269542016186774
Normal: h_loss: 0.032852615921878034 macro F 0.344361970446369 micro F 0.5046318868844465 micro P 0.6925393108062897 micro R 0.3969319271332694
Multi only: h_loss: 0.05720650151767356 macro F 0.3060954130077195 micro F 0.44184380224504416
Jaccard: 0.39875132035611927
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09275336151764695
Normal: h_loss: 0.03250097005755675 macro F 0.33826695810353485 micro F 0.5403304178814383 micro P 0.6691207702109585 micro R 0.45311601150527325
Multi only: h_loss: 0.06139234309213747 macro F 0.28654549574784965 micro F 0.41456582633053224
Jaccard: 0.46961106081183074
saving best model ...
Training Loss for epoch 1: 0.09308554458359014
Evaluating:
Evaluation results:
Evaluation Loss 0.09289468031573911
Normal: h_loss: 0.03205636034404708 macro F 0.3069520570334646 micro F 0.5299033845059569 micro P 0.6939916162086632 micro R 0.42857142857142855
Multi only: h_loss: 0.06070694213257613 macro F 0.2652287084467507 micro F 0.4078319006685769
Jaccard: 0.44641617624867996
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031693384927215774 macro F 0.3123807552506484 micro F 0.5275652344516383
Multi only: h_loss: 0.060889230244068955 macro F 0.2468149615473077 micro F 0.4016771488469602
Jaccard: 0.4394846753884896
Evaluating:
23500
23500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09291396986215888
Normal: h_loss: 0.03204827653107418 macro F 0.3586652260729769 micro F 0.5264289553843396 micro P 0.6980833201330587 micro R 0.4225311601150527
Multi only: h_loss: 0.059115832762165865 macro F 0.30012069430440663 micro F 0.41793203181489513
Jaccard: 0.4349347366832657
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09506847972498052
Normal: h_loss: 0.033188094160253506 macro F 0.3495619458158149 micro F 0.534866594913046 micro P 0.6536065346808805 micro R 0.4526366251198466
Multi only: h_loss: 0.057867423871536275 macro F 0.3148788543815505 micro F 0.46052031036056595
Jaccard: 0.4527689754036522
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09273828342118486
Normal: h_loss: 0.03225037185539675 macro F 0.34242197686054415 micro F 0.5191925278698403 micro P 0.6987834549878346 micro R 0.41303930968360497
Multi only: h_loss: 0.05931166160775482 macro F 0.29248195868747817 micro F 0.41656633758728634
Jaccard: 0.42600347065037
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09166572147186874
Normal: h_loss: 0.032040192718101275 macro F 0.3698403174086012 micro F 0.5352641144398195 micro P 0.6888486494643127 micro R 0.437679769894535
Multi only: h_loss: 0.06026632723000098 macro F 0.2957968820442926 micro F 0.41044061302681994
Jaccard: 0.45453259393390705
saving best model ...
Training Loss for epoch 1: 0.08849009376637479
Evaluating:
Evaluation results:
Evaluation Loss 0.09269566189593835
Normal: h_loss: 0.03257372437431288 macro F 0.3514190856199603 micro F 0.5409285103958985 micro P 0.6663859649122807 micro R 0.45522531160115054
Multi only: h_loss: 0.06024184862430236 macro F 0.29084702768853304 micro F 0.426206574959198
Jaccard: 0.4685943866002718
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03245018294769539 macro F 0.3508853318504301 micro F 0.5400615614215092
Multi only: h_loss: 0.05986516470387438 macro F 0.29145874997763205 micro F 0.4294428629524197
Jaccard: 0.4658344082058843
Evaluating:
24000
24000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09562305767121171
Normal: h_loss: 0.03344273426889995 macro F 0.39322064032872944 micro F 0.5360547269260962 micro P 0.6455969746083198 micro R 0.4582933844678811
Multi only: h_loss: 0.057157544306276314 macro F 0.34449370056386475 micro F 0.4699205448354142
Jaccard: 0.4551305266334697
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09598782629907913
Normal: h_loss: 0.03337402185863028 macro F 0.38930950869940034 micro F 0.5323137921268762 micro P 0.6503806228373702 micro R 0.4505273250239693
Multi only: h_loss: 0.057084108489180455 macro F 0.3405077801667145 micro F 0.46611721611721607
Jaccard: 0.4506564056134001
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09497001473047767
Normal: h_loss: 0.03222207850999159 macro F 0.33979836376776495 micro F 0.5386574074074073 micro P 0.6794160583941606 micro R 0.4462128475551294
Multi only: h_loss: 0.05921374718496034 macro F 0.2833180811535148 micro F 0.4263694569599241
Jaccard: 0.46339784216085733
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09258938222556914
Normal: h_loss: 0.03258180818728578 macro F 0.39203370879957555 micro F 0.5432084773615913 micro P 0.6641263682970764 micro R 0.45953978906999043
Multi only: h_loss: 0.05757368060315284 macro F 0.33018938509266704 micro F 0.4585635359116022
Jaccard: 0.46244529953221714
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.08327478714783987
Evaluating:
Evaluation results:
Evaluation Loss 0.09237696709016051
Normal: h_loss: 0.03225845566836966 macro F 0.38704687052508574 micro F 0.5290056063735615 micro P 0.6879508825786647 micro R 0.42972195589645257
Multi only: h_loss: 0.05735337315186527 macro F 0.3284869335562018 micro F 0.4480565371024735
Jaccard: 0.43780179568432226
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.032121140330095554 macro F 0.38448269717473604 micro F 0.5257020697697016
Multi only: h_loss: 0.05815838880355009 macro F 0.31383506992320026 micro F 0.44116441164411646
Jaccard: 0.4332657699158528
Evaluating:
24500
24500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09960348676466844
Normal: h_loss: 0.03402072689646252 macro F 0.41899868510300525 micro F 0.5434228369948466 micro P 0.625733916302311 micro R 0.48024928092042185
Multi only: h_loss: 0.05838147459120729 macro F 0.37580501543060774 micro F 0.4677527337647846
Jaccard: 0.47429078014184456
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09632253549881735
Normal: h_loss: 0.033256806570523184 macro F 0.4066548830328865 micro F 0.5332955190017017 micro P 0.6529166666666667 micro R 0.45071907957814
Multi only: h_loss: 0.058724175070987955 macro F 0.3360703408676781 micro F 0.4483789376868246
Jaccard: 0.456092500377245
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09670024380027141
Normal: h_loss: 0.033260848477009636 macro F 0.40261702881169575 micro F 0.5289369740683497 micro P 0.6563432305725245 micro R 0.4429530201342282
Multi only: h_loss: 0.05823460295701557 macro F 0.3417932153258669 micro F 0.451969592259848
Jaccard: 0.4464972838388416
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09768228599993067
Normal: h_loss: 0.03351548858565608 macro F 0.4068028194080986 micro F 0.5320013545546901 micro P 0.6466794731064764 micro R 0.45186960690316397
Multi only: h_loss: 0.05845491040830314 macro F 0.34920366745515163 micro F 0.45578851412944393
Jaccard: 0.4531499924551085
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.07903828411807819
Evaluating:
Evaluation results:
Evaluation Loss 0.0970472612512252
Normal: h_loss: 0.033022375994308996 macro F 0.3976187061111687 micro F 0.544694605439144 micro P 0.650385946233697 micro R 0.4685522531160115
Multi only: h_loss: 0.05884656809948105 macro F 0.3353956457561713 micro F 0.45164233576642343
Jaccard: 0.47494152708616316
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03273974045118324 macro F 0.39359055298096707 micro F 0.5470272238914686
Multi only: h_loss: 0.0587557603686636 macro F 0.3294727165428165 micro F 0.4533545057562525
Jaccard: 0.47764265094281694
Evaluating:
25000
25000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.1044347052082534
Normal: h_loss: 0.034364288947810906 macro F 0.41387007915102497 micro F 0.5395363951473138 micro P 0.6199900423201394 micro R 0.4775647171620326
Multi only: h_loss: 0.05962988348183688 macro F 0.3686713383427472 micro F 0.45600714604734255
Jaccard: 0.4739927569035768
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.102808341176505
Normal: h_loss: 0.034364288947810906 macro F 0.4209233811617255 micro F 0.5308982564555287 micro P 0.6252924356641539 micro R 0.4612655800575264
Multi only: h_loss: 0.05740233036326251 macro F 0.3852891577267525 micro F 0.478541249722037
Jaccard: 0.4531349026708924
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10659344228226549
Normal: h_loss: 0.03332147707430641 macro F 0.4092033578098258 micro F 0.552879921900423 micro P 0.6364885114885115 micro R 0.48868648130393094
Multi only: h_loss: 0.058797610888083815 macro F 0.35615590093030114 micro F 0.4671694764862466
Jaccard: 0.4893918816960923
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1032739783969871
Normal: h_loss: 0.03385096682403156 macro F 0.3896800647175045 micro F 0.523090940151472 micro P 0.6440891880521666 micro R 0.44036433365292427
Multi only: h_loss: 0.05921374718496034 macro F 0.33500475422031073 micro F 0.4422411805395435
Jaccard: 0.4434416025350846
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.07153272968053817
Evaluating:
Evaluation results:
Evaluation Loss 0.10178508858954059
Normal: h_loss: 0.0339560563926793 macro F 0.4041301260501216 micro F 0.5394441094238254 micro P 0.6298809371399309 micro R 0.47171620325982744
Multi only: h_loss: 0.05916478997356311 macro F 0.34997034791055887 micro F 0.4537853107344633
Jaccard: 0.4708012675418748
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03341756824343889 macro F 0.40138683650190715 micro F 0.5477377983612398
Multi only: h_loss: 0.05858508277863116 macro F 0.3413301599570442 micro F 0.46555079797586607
Jaccard: 0.4822185369449053
