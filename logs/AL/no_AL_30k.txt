
loading file
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.15033814192303924
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.15000861499335444
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1407071837355185
Normal: h_loss: 0.040216969540192715 macro F 0.03377362534615821 micro F 0.23672905799324945 micro P 0.5920951650038373 micro R 0.1479386385426654
Multi only: h_loss: 0.07446391853520024 macro F 0.030846173575078423 micro F 0.09518143961927425
Jaccard: 0.1654217594688396
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.13013069336367927
Normal: h_loss: 0.03798583715967147 macro F 0.06566183073860032 micro F 0.32173787528868364 micro P 0.6506129597197898 micro R 0.2137104506232023
Multi only: h_loss: 0.07106139234309214 macro F 0.060310935664022125 micro F 0.18110014104372354
Jaccard: 0.2334955485136562
saving best model ...
Training Loss for epoch 1: 0.15269236884911855
Evaluating:
Evaluation results:
Evaluation Loss 0.13037139470083997
Normal: h_loss: 0.03858808122615275 macro F 0.05553787858876756 micro F 0.3617888896316599 micro P 0.5974828880547582 micro R 0.2594439117929051
Multi only: h_loss: 0.07321550964457064 macro F 0.050441350591563676 micro F 0.17852238396045042
Jaccard: 0.2874320959710276
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.038215009608044435 macro F 0.05520685594788726 micro F 0.36417387495894005
Multi only: h_loss: 0.07292200034135518 macro F 0.05116237274921431 micro F 0.1873514027579648
Jaccard: 0.2879737116884712
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.12162110468498341
Normal: h_loss: 0.03650245747914376 macro F 0.08844378298698226 micro F 0.37635522408673433 micro P 0.6726734139718588 micro R 0.26126558005752637
Multi only: h_loss: 0.06973954763536669 macro F 0.07922712839771616 micro F 0.21102187759623373
Jaccard: 0.2862626376942809
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.11405313087499967
Normal: h_loss: 0.0351322511802367 macro F 0.11623573690095435 micro F 0.38284578244816814 micro P 0.7378215654077723 micro R 0.2584851390220518
Multi only: h_loss: 0.06743855869969646 macro F 0.10115601317508462 micro F 0.2366306456082017
Jaccard: 0.2791327146521804
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10942665531037349
Normal: h_loss: 0.036175063053741185 macro F 0.17074915753612027 micro F 0.48851297291118984 micro P 0.6046972269383135 micro R 0.4097794822627037
Multi only: h_loss: 0.06648389307745031 macro F 0.16677929931844337 micro F 0.35023923444976074
Jaccard: 0.42109929078014213
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10345825195083147
Normal: h_loss: 0.033903511608355426 macro F 0.20304570052074894 micro F 0.47744829304759523 micro P 0.6816079686944148 micro R 0.3674017257909875
Multi only: h_loss: 0.06109859982375404 macro F 0.19593468788194718 micro F 0.3849186791522918
Jaccard: 0.37457937226497684
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.11454699724912644
Evaluating:
Evaluation results:
Evaluation Loss 0.10219125858107106
Normal: h_loss: 0.03399243355105736 macro F 0.21272349153135656 micro F 0.5038933459178858 micro P 0.6548604722477768 micro R 0.40949185043144776
Multi only: h_loss: 0.0625673161656712 macro F 0.20264084972478255 micro F 0.3808139534883721
Jaccard: 0.42092387203863013
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03353602358577483 macro F 0.21233403562583372 micro F 0.5052427184466018
Multi only: h_loss: 0.060675883256528416 macro F 0.20529829665459937 micro F 0.40651085141903176
Jaccard: 0.4162827836128002
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10248858734607805
Normal: h_loss: 0.033543781931061244 macro F 0.17113160334731362 micro F 0.45108803492294464 micro P 0.72723395180209 micro R 0.32694151486097794
Multi only: h_loss: 0.06347302457652013 macro F 0.16440527408846342 micro F 0.33049315775884325
Jaccard: 0.3443281273577788
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0998716186302018
Normal: h_loss: 0.033119381749983835 macro F 0.21487343572439194 micro F 0.4719680371181853 micro P 0.7197327044025157 micro R 0.3511025886864813
Multi only: h_loss: 0.060829335161069226 macro F 0.2102260467956214 micro F 0.381994528724198
Jaccard: 0.36089482420401403
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0985538268781399
Normal: h_loss: 0.033042585526741254 macro F 0.22018629201509346 micro F 0.48223446703401107 micro P 0.7103937301735398 micro R 0.36500479386385426
Multi only: h_loss: 0.062420444531479484 macro F 0.20145835844425847 micro F 0.3544303797468355
Jaccard: 0.3821714199486948
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09534769840521638
Normal: h_loss: 0.03260201771971804 macro F 0.2505703291245439 micro F 0.5130991186768079 micro P 0.6926336375488917 micro R 0.4074784276126558
Multi only: h_loss: 0.06141682169783609 macro F 0.22135880377980424 micro F 0.3911671924290221
Jaccard: 0.4257431718726424
saving best model ...
Training Loss for epoch 1: 0.09911696099440256
Evaluating:
Evaluation results:
Evaluation Loss 0.09631923282238194
Normal: h_loss: 0.033066836965659964 macro F 0.22484688621518428 micro F 0.5189062040576301 micro P 0.6710266159695818 micro R 0.42301054650047937
Multi only: h_loss: 0.062420444531479484 macro F 0.20405690860607215 micro F 0.39111747851002865
Jaccard: 0.4439829485438359
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.0326147042564953 macro F 0.224433356993552 micro F 0.5190217391304348
Multi only: h_loss: 0.060889230244068955 macro F 0.2001393032346806 micro F 0.40067198656026887
Jaccard: 0.44011424359683066
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.0932022353247206
Normal: h_loss: 0.032254413761883205 macro F 0.28525948201015 micro F 0.5175915850562205 micro P 0.700425392670157 micro R 0.4104506232023011
Multi only: h_loss: 0.059899148144521686 macro F 0.2528026148864836 micro F 0.40736255751998063
Jaccard: 0.4235645842764452
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0921988438776091
Normal: h_loss: 0.032254413761883205 macro F 0.28873571483019317 micro F 0.5192771084337349 micro P 0.6985413290113452 micro R 0.41323106423777567
Multi only: h_loss: 0.0609272495838637 macro F 0.2528635101844641 micro F 0.3966060606060606
Jaccard: 0.430689791760978
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09181391555038498
Normal: h_loss: 0.03186234883269741 macro F 0.28597706852897425 micro F 0.5135452020981178 micro P 0.7205194805194806 micro R 0.3989453499520614
Multi only: h_loss: 0.06065798492117889 macro F 0.2529381162276902 micro F 0.38905325443786987
Jaccard: 0.41890184095367444
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09053024331899508
Normal: h_loss: 0.032068486063506436 macro F 0.2869781550840859 micro F 0.5366736743751459 micro P 0.6864356139826711 micro R 0.4405560882070949
Multi only: h_loss: 0.05994810535591893 macro F 0.24840887083558763 micro F 0.41870401139330643
Jaccard: 0.45471178512147303
saving best model ...
Training Loss for epoch 1: 0.08979147457679112
Evaluating:
Evaluation results:
Evaluation Loss 0.09076635165147622
Normal: h_loss: 0.03231908426566643 macro F 0.2857204014264098 micro F 0.5289265936137624 micro P 0.6859718826405868 micro R 0.43039309683604987
Multi only: h_loss: 0.05994810535591893 macro F 0.25178424867048765 micro F 0.4167658966420577
Jaccard: 0.4434698958804893
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03156176788017584 macro F 0.2963846331013253 micro F 0.5337351740229438
Multi only: h_loss: 0.05854241338112306 macro F 0.2551435290895981 micro F 0.4311774461028193
Jaccard: 0.44154228855721384
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.0901812590850916
Normal: h_loss: 0.03178959451594128 macro F 0.3310622974812939 micro F 0.5352478874903978 micro P 0.6975204065917141 micro R 0.4342281879194631
Multi only: h_loss: 0.05772055223734456 macro F 0.2998143071613303 micro F 0.44673862036602535
Jaccard: 0.44460351591972264
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09010067104031835
Normal: h_loss: 0.03185830692621096 macro F 0.31068617584141095 micro F 0.5166789305862154 micro P 0.7167403878870364 micro R 0.4039309683604986
Multi only: h_loss: 0.05936061881915206 macro F 0.2624071408576457 micro F 0.40925700365408046
Jaccard: 0.4175098083597408
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08922637733278797
Normal: h_loss: 0.031825971674319345 macro F 0.35268828745562875 micro F 0.5360047142015322 micro P 0.6954128440366972 micro R 0.4360498561840844
Multi only: h_loss: 0.05896896112797415 macro F 0.3047592715744905 micro F 0.4284697508896797
Jaccard: 0.44931718726422243
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08844280694373165
Normal: h_loss: 0.0314541162775658 macro F 0.3615487797307141 micro F 0.5494441871236684 micro P 0.6935106693949138 micro R 0.4549376797698945
Multi only: h_loss: 0.05740233036326251 macro F 0.31442326437215534 micro F 0.45043355987813455
Jaccard: 0.46697034857401554
saving best model ...
Training Loss for epoch 1: 0.08373158676624298
Evaluating:
Evaluation results:
Evaluation Loss 0.08843044748115778
Normal: h_loss: 0.03164408588242902 macro F 0.36426245605711544 micro F 0.5337938426725422 micro P 0.7043847241867044 micro R 0.42972195589645257
Multi only: h_loss: 0.058405953196905905 macro F 0.3086339228356282 micro F 0.42891335567257055
Jaccard: 0.44390184095367463
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03133143804785596 macro F 0.3618592787392981 micro F 0.5316281357599607
Multi only: h_loss: 0.05845707458610684 macro F 0.2883439809821327 micro F 0.42485306465155337
Jaccard: 0.43767888950310185
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09136198815540147
Normal: h_loss: 0.0315794153786458 macro F 0.37202044509732246 micro F 0.5627623258156584 micro P 0.675897298023928 micro R 0.4820709491850431
Multi only: h_loss: 0.0595319690590424 macro F 0.3052697434720743 micro F 0.4367762853172765
Jaccard: 0.499801946582164
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08802704340552733
Normal: h_loss: 0.03151878678134903 macro F 0.390753221372547 micro F 0.5607750366114679 micro P 0.6796832332058984 micro R 0.4772770853307766
Multi only: h_loss: 0.05784294526583766 macro F 0.3404396478598362 micro F 0.456156501726122
Jaccard: 0.48903349932095974
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08700645695941758
Normal: h_loss: 0.03126818857918903 macro F 0.35470419926403135 micro F 0.5382043935052531 micro P 0.713065485605821 micro R 0.43221476510067114
Multi only: h_loss: 0.05896896112797415 macro F 0.29757859832219086 micro F 0.4199373946544667
Jaccard: 0.4486230571902826
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08807836994427586
Normal: h_loss: 0.03155516393972709 macro F 0.38257478114044285 micro F 0.5598466482494221 micro P 0.6794854249349939 micro R 0.47603068072866733
Multi only: h_loss: 0.058871046705179675 macro F 0.32019099675386575 micro F 0.4408277144850034
Jaccard: 0.4914742719179116
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.07786424589157105
Evaluating:
Evaluation results:
Evaluation Loss 0.08820618728522317
Normal: h_loss: 0.031825971674319345 macro F 0.3931386308380948 micro F 0.5381276396058189 micro P 0.6931097008159565 micro R 0.4397890699904123
Multi only: h_loss: 0.05735337315186527 macro F 0.33104655257836263 micro F 0.4496124031007752
Jaccard: 0.4504149690659428
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031430150833135904 macro F 0.3919529970020195 micro F 0.5420901246404602
Multi only: h_loss: 0.05692097627581499 macro F 0.31599643680320216 micro F 0.45372645372645376
Jaccard: 0.4536269270929307
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.0895123512476966
Normal: h_loss: 0.03155516393972709 macro F 0.3767466306153174 micro F 0.5603919139591194 micro P 0.6789466502933552 micro R 0.4770853307766059
Multi only: h_loss: 0.05818564574561833 macro F 0.3246401036282879 micro F 0.45243031559548497
Jaccard: 0.49183642673909755
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09104136936113597
Normal: h_loss: 0.031991689840263855 macro F 0.32880217666937817 micro F 0.5476367377264674 micro P 0.6779397198245366 micro R 0.45934803451581974
Multi only: h_loss: 0.05955644766474102 macro F 0.2700752643688072 micro F 0.4303441816904706
Jaccard: 0.47592236306020846
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09059574918280958
Normal: h_loss: 0.03186639073918386 macro F 0.39904643176437204 micro F 0.5444880979893691 micro P 0.6850828729281768 micro R 0.45177372962607865
Multi only: h_loss: 0.05576226378145501 macro F 0.35443058813845785 micro F 0.47704315886134063
Jaccard: 0.45490417987022813
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08892136392967363
Normal: h_loss: 0.0314460324645929 macro F 0.3784420907370542 micro F 0.5486715396217658 micro P 0.6946239717978848 micro R 0.4534036433365292
Multi only: h_loss: 0.05830803877411143 macro F 0.32141599327613235 micro F 0.44371788883699204
Jaccard: 0.46515014335295035
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.076523197889328
Evaluating:
Evaluation results:
Evaluation Loss 0.08983571838204707
Normal: h_loss: 0.03167237922783418 macro F 0.33741108051830315 micro F 0.5344028520499109 micro P 0.70265625 micro R 0.4311601150527325
Multi only: h_loss: 0.06112307842945266 macro F 0.27110534865261904 micro F 0.3911241160692514
Jaccard: 0.4539855892560738
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031127431624944062 macro F 0.3463389807296302 micro F 0.5381761374731497
Multi only: h_loss: 0.05952380952380952 macro F 0.27645075082677106 micro F 0.41213653603034134
Jaccard: 0.4525981205085683
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.08961715791905665
Normal: h_loss: 0.032521179589989006 macro F 0.40053118332525256 micro F 0.540963030579644 micro P 0.667934629473091 micro R 0.45455417066155324
Multi only: h_loss: 0.055909135415646724 macro F 0.35851905676255347 micro F 0.47397512666973746
Jaccard: 0.45595480609627315
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08991902823960042
Normal: h_loss: 0.031680463040807086 macro F 0.354869544568149 micro F 0.5566240524946261 micro P 0.6788079470198676 micro R 0.47171620325982744
Multi only: h_loss: 0.05830803877411143 macro F 0.3036686844625333 micro F 0.44912118408880664
Jaccard: 0.4821601026105329
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0889309299979047
Normal: h_loss: 0.03142178102567419 macro F 0.39822623531438245 micro F 0.5505838825297722 micro P 0.6933605125218404 micro R 0.45656759348034515
Multi only: h_loss: 0.05727993733476941 macro F 0.35319183612526955 micro F 0.4542910447761193
Jaccard: 0.4673438207333638
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08876377438570698
Normal: h_loss: 0.03162387634999677 macro F 0.3602215546683893 micro F 0.5610412926391383 micro P 0.676223965377333 micro R 0.4793863854266539
Multi only: h_loss: 0.05742680896896113 macro F 0.309732534770959 micro F 0.46560364464692483
Jaccard: 0.4888505356873401
saving best model ...
Training Loss for epoch 1: 0.07549292042652767
Evaluating:
Evaluation results:
Evaluation Loss 0.08798061297922288
Normal: h_loss: 0.0314460324645929 macro F 0.37632918928845754 micro F 0.5549199084668193 micro P 0.6879432624113475 micro R 0.4650047938638543
Multi only: h_loss: 0.0571330657005777 macro F 0.31944617836807243 micro F 0.45947197776748494
Jaccard: 0.477054096876415
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.030910263497328175 macro F 0.38486674286630873 micro F 0.5582620144832127
Multi only: h_loss: 0.056366274108209595 macro F 0.30729458214910305 micro F 0.4653986240388507
Jaccard: 0.47667526564707335
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09173368858219866
Normal: h_loss: 0.031943186962426436 macro F 0.41252547532629774 micro F 0.5589597633796529 micro P 0.6687141140339165 micro R 0.48015340364333653
Multi only: h_loss: 0.05735337315186527 macro F 0.3650010043506189 micro F 0.46762099522835715
Jaccard: 0.48693036064584294
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09127803576200351
Normal: h_loss: 0.0322463299489103 macro F 0.39988759530199874 micro F 0.5492655367231639 micro P 0.6686382393397524 micro R 0.4660594439117929
Multi only: h_loss: 0.057622637814550084 macro F 0.34447675809440315 micro F 0.4566020313942752
Jaccard: 0.47440395352346487
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09267652664298168
Normal: h_loss: 0.03236758714350385 macro F 0.3896791081334762 micro F 0.5478260869565218 micro P 0.6663461538461538 micro R 0.4651006711409396
Multi only: h_loss: 0.05850386761970038 macro F 0.3340755419900915 micro F 0.4462465245597776
Jaccard: 0.47697298928625337
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09032324331210932
Normal: h_loss: 0.03192297742999418 macro F 0.4093130492970672 micro F 0.5470291351227347 micro P 0.6807022552098202 micro R 0.45723873441994245
Multi only: h_loss: 0.05869969646528934 macro F 0.3475159759126149 micro F 0.4394576905095839
Jaccard: 0.47062396257733546
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.06868398923476537
Evaluating:
Evaluation results:
Evaluation Loss 0.09238065218307574
Normal: h_loss: 0.03267073012998771 macro F 0.39929077858160983 micro F 0.5581369922921335 micro P 0.6492432913646191 micro R 0.4894534995206136
Multi only: h_loss: 0.059580926270439635 macro F 0.33904497918999627 micro F 0.44757149341806635
Jaccard: 0.49785159197223505
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.032114559477743554 macro F 0.4069424601206533 micro F 0.5650623885918004
Multi only: h_loss: 0.05837173579109063 macro F 0.3274484958737676 micro F 0.4631083202511773
Jaccard: 0.5041981450770839
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09847813925122773
Normal: h_loss: 0.03232312617215288 macro F 0.396305988839214 micro F 0.5458056454819107 micro P 0.6694997909990247 micro R 0.4606903163950144
Multi only: h_loss: 0.058528346225399 macro F 0.3415626846310728 micro F 0.4481883221786291
Jaccard: 0.4710464765353858
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0991932354756155
Normal: h_loss: 0.03258180818728578 macro F 0.4100503048973149 micro F 0.5657022789720381 micro P 0.6456770384946501 micro R 0.5033557046979866
Multi only: h_loss: 0.057549201997454225 macro F 0.35789359479318245 micro F 0.47952180650874476
Jaccard: 0.5061717217443791
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09767946711374172
Normal: h_loss: 0.033159800814848345 macro F 0.406481141658992 micro F 0.5512035010940919 micro P 0.6417834394904458 micro R 0.48302972195589644
Multi only: h_loss: 0.05718202291197493 macro F 0.3648259869804442 micro F 0.48042704626334515
Jaccard: 0.48229591066847755
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09987640811063792
Normal: h_loss: 0.033091088404578674 macro F 0.40275464033911396 micro F 0.5572440646801147 micro P 0.6391266592234214 micro R 0.49395973154362416
Multi only: h_loss: 0.059189268579261725 macro F 0.349473419718281 micro F 0.4612299465240642
Jaccard: 0.49865889542779557
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.06086474088629087
Evaluating:
Evaluation results:
Evaluation Loss 0.09754976577594734
Normal: h_loss: 0.0334386923624135 macro F 0.40069405516729006 micro F 0.549719697382028 micro P 0.6357799320156112 micro R 0.4841802492809204
Multi only: h_loss: 0.05784294526583766 macro F 0.3428205913727763 micro F 0.4686305374409715
Jaccard: 0.4866304511845483
patience 3 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03262786596119929 macro F 0.4171688388869326 micro F 0.5591321358705318
Multi only: h_loss: 0.055811571940604196 macro F 0.36652036567478635 micro F 0.4906542056074766
Jaccard: 0.49495424113997905
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10008681578677182
Normal: h_loss: 0.033208303692685764 macro F 0.40682482918782864 micro F 0.5547366139171905 micro P 0.6379955123410621 micro R 0.4906999041227229
Multi only: h_loss: 0.06019289141290512 macro F 0.34996354674273267 micro F 0.4480359147025814
Jaccard: 0.4982873094914746
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10200592623192997
Normal: h_loss: 0.03358824290241221 macro F 0.40670814826376817 micro F 0.557272242940863 micro P 0.6270983213429256 micro R 0.50143815915628
Multi only: h_loss: 0.05784294526583766 macro F 0.3511608703129189 micro F 0.47985912392692054
Jaccard: 0.49889467330617215
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09992996353829754
Normal: h_loss: 0.033236597038090926 macro F 0.41918102092025694 micro F 0.5434456720892787 micro P 0.6455612716000527 micro R 0.4692233940556088
Multi only: h_loss: 0.057940859688632135 macro F 0.36813326468397817 micro F 0.4648428668324667
Jaccard: 0.475667722951562
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10166944808762178
Normal: h_loss: 0.03418240315592058 macro F 0.43104450429809843 micro F 0.5563191857719952 micro P 0.6142973004286872 micro R 0.5083413231064238
Multi only: h_loss: 0.05798981690002938 macro F 0.38800545528010427 micro F 0.4891093379340091
Jaccard: 0.4999396408631363
saving best model ...
Training Loss for epoch 1: 0.056810229897499086
Evaluating:
Evaluation results:
Evaluation Loss 0.10105230304930694
Normal: h_loss: 0.03324468085106383 macro F 0.4219900377721954 micro F 0.5586262409444593 micro P 0.6343692870201096 micro R 0.4990412272291467
Multi only: h_loss: 0.05816116713991971 macro F 0.36856782354274736 micro F 0.4748010610079576
Jaccard: 0.5011694582767472
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.032746321303535234 macro F 0.43413047014031897 micro F 0.5628952916373857
Multi only: h_loss: 0.05683563748079877 macro F 0.3580358812445352 micro F 0.4825174825174826
Jaccard: 0.5045758860020885
