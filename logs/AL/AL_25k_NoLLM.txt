
loading file
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.15312628885128207
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.15125447172366316
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1486573496208627
Normal: h_loss: 0.0413608290758585 macro F 0.009166046456970504 micro F 0.0838033843674456 micro P 0.6332882273342354 micro R 0.0448705656759348
Multi only: h_loss: 0.07495349064917263 macro F 0.005173099880620771 micro F 0.016698779704560053
Jaccard: 0.05147502640712238
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.14180444600545428
Normal: h_loss: 0.0413527452628856 macro F 0.020172117039586917 micro F 0.25576489415872555 micro P 0.5299969852276153 micro R 0.16855225311601152
Multi only: h_loss: 0.07664251444237737 macro F 0.01596638655462185 micro F 0.08850072780203785
Jaccard: 0.1902821789648408
saving best model ...
Training Loss for epoch 1: 0.1606113401889801
Evaluating:
Evaluation results:
Evaluation Loss 0.14117738450069997
Normal: h_loss: 0.04113448231261722 macro F 0.020141707528799006 micro F 0.25229593710969067 micro P 0.5397673687519647 micro R 0.16462128475551294
Multi only: h_loss: 0.07629981396259669 macro F 0.01606600086843248 micro F 0.08672721945502491
Jaccard: 0.18588727931190582
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.04040643344125931 macro F 0.02042183432858674 micro F 0.2579163645153493
Multi only: h_loss: 0.07620754394947943 macro F 0.015357142857142857 micro F 0.08784473953013278
Jaccard: 0.18859406670351947
Evaluating:
20500
20500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.13308412541545533
Normal: h_loss: 0.03929541486128177 macro F 0.04166149088237503 micro F 0.17047781569965872 micro P 0.7744186046511627 micro R 0.09578139980824545
Multi only: h_loss: 0.07145305003427005 macro F 0.03860621874320504 micro F 0.10156971375807941
Jaccard: 0.1034310396861325
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1290904846381364
Normal: h_loss: 0.039311582487227575 macro F 0.05874495680336702 micro F 0.1858362631843295 micro P 0.7321899736147758 micro R 0.10642377756471716
Multi only: h_loss: 0.07066973465191423 macro F 0.05325684756005664 micro F 0.12594610959733576
Jaccard: 0.11303191489361705
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.12205496848290621
Normal: h_loss: 0.036110392549957966 macro F 0.09857838727832303 micro F 0.3748075577326802 micro P 0.6937823834196891 micro R 0.25675934803451583
Multi only: h_loss: 0.06890727504161363 macro F 0.08810337245247415 micro F 0.23066411587865535
Jaccard: 0.2778972385694884
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.11833553020570439
Normal: h_loss: 0.03657925370238634 macro F 0.1069043013489348 micro F 0.32603515043193326 micro P 0.7301534356237491 micro R 0.20987535953978906
Multi only: h_loss: 0.06783021639087437 macro F 0.0925066237614535 micro F 0.20986598232107212
Jaccard: 0.22565829183642663
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.13007467699923167
Evaluating:
Evaluation results:
Evaluation Loss 0.11991487611076744
Normal: h_loss: 0.03683793571751924 macro F 0.11381137344315517 micro F 0.38848631239935594 micro P 0.6470719713902549 micro R 0.2775647171620326
Multi only: h_loss: 0.06753647312249095 macro F 0.1080424863945196 micro F 0.26875165650675853
Jaccard: 0.29522219707258185
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.035872226170733636 macro F 0.11366276804532598 micro F 0.39627865765865544
Multi only: h_loss: 0.06626557433009046 macro F 0.11030696099462294 micro F 0.2853198343304188
Jaccard: 0.297024138566427
Evaluating:
21000
21000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.11421837156487569
Normal: h_loss: 0.03570215999482636 macro F 0.14014825638551073 micro F 0.3987475325028929 micro P 0.6873973245716968 micro R 0.2808245445829338
Multi only: h_loss: 0.0644521688044649 macro F 0.13205989895428985 micro F 0.30765185379963184
Jaccard: 0.29143088878829027
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10729644138613886
Normal: h_loss: 0.034400666106188964 macro F 0.14858154835391474 micro F 0.4268300895683211 micro P 0.7171305725277212 micro R 0.3038350910834132
Multi only: h_loss: 0.06469695486145109 macro F 0.13963324655814893 micro F 0.3075713911448782
Jaccard: 0.31929983401237366
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10579806056603851
Normal: h_loss: 0.0342228222207851 macro F 0.13256985359709347 micro F 0.4368473561689391 micro P 0.7131378935939197 micro R 0.31486097794822626
Multi only: h_loss: 0.06555370606090277 macro F 0.12001026321168917 micro F 0.2934036939313984
Jaccard: 0.3356722498868266
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10255626582651453
Normal: h_loss: 0.03363674578024963 macro F 0.18365983278352369 micro F 0.4344930687686872 micro P 0.745916938870742 micro R 0.3065196548418025
Multi only: h_loss: 0.06261627337706845 macro F 0.16909141974008754 micro F 0.33350703491401773
Jaccard: 0.31937528293345413
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.11055696818658284
Evaluating:
Evaluation results:
Evaluation Loss 0.10339168327299379
Normal: h_loss: 0.03384692491754511 macro F 0.186679919903897 micro F 0.46306745319312637 micro P 0.6989934185056136 micro R 0.34621284755512943
Multi only: h_loss: 0.062322530108685006 macro F 0.17905285553398587 micro F 0.36158475426278835
Jaccard: 0.35915572657311023
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.033522861881070835 macro F 0.1833779494840975 micro F 0.4578544061302683
Multi only: h_loss: 0.06229732036183649 macro F 0.17611734002801932 micro F 0.3551236749116608
Jaccard: 0.3510687304219642
Evaluating:
21500
21500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10037797176767767
Normal: h_loss: 0.033353812326198023 macro F 0.20245539517746566 micro F 0.4556728232189974 micro P 0.7302325581395349 micro R 0.3311601150527325
Multi only: h_loss: 0.0629344952511505 macro F 0.17708655366588388 micro F 0.33238119968839264
Jaccard: 0.3488852421910367
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0991665222131866
Normal: h_loss: 0.03376608678781608 macro F 0.2125996625006055 micro F 0.5098568411171087 micro P 0.6569398246144542 micro R 0.41658676893576224
Multi only: h_loss: 0.06327719573093117 macro F 0.18985216022697454 micro F 0.37424352457032195
Jaccard: 0.43640599064433394
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09625969701368596
Normal: h_loss: 0.033159800814848345 macro F 0.23997497255796468 micro F 0.502426006792819 micro P 0.6837240013205679 micro R 0.39712368168744006
Multi only: h_loss: 0.0626897091941643 macro F 0.21575111063922578 micro F 0.3674981476907878
Jaccard: 0.4180096574618986
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09656716539828472
Normal: h_loss: 0.03276773588566255 macro F 0.2629938301087539 micro F 0.4682190882256478 micro P 0.7412253374870197 micro R 0.34218600191754556
Multi only: h_loss: 0.061294428669342996 macro F 0.23065745807639265 micro F 0.3618756371049949
Jaccard: 0.3571789648408029
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.0978449082915173
Evaluating:
Evaluation results:
Evaluation Loss 0.09554691633969316
Normal: h_loss: 0.03268689775593352 macro F 0.23453621327257315 micro F 0.511329989727476 micro P 0.6914528517731655 micro R 0.4056567593480345
Multi only: h_loss: 0.06271418779986292 macro F 0.20516999963058094 micro F 0.3739002932551319
Jaccard: 0.42860079975856363
patience 3 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.032239595672431495 macro F 0.2393027507527714 micro F 0.5114191682457365
Multi only: h_loss: 0.06221198156682028 macro F 0.197056833981412 micro F 0.36992221261884184
Jaccard: 0.42485105337509976
Evaluating:
22000
22000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09812049486414239
Normal: h_loss: 0.0331719265343077 macro F 0.2588879071363049 micro F 0.5233753411928684 micro P 0.6637207247017234 micro R 0.4320230105465005
Multi only: h_loss: 0.061612650543425046 macro F 0.2270241653999518 micro F 0.3933477946493131
Jaccard: 0.44628036819073535
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09772149118775693
Normal: h_loss: 0.033066836965659964 macro F 0.27020710808598203 micro F 0.47868476390747466 micro P 0.7136614098422953 micro R 0.3601150527325024
Multi only: h_loss: 0.06048663468128855 macro F 0.2305896044433616 micro F 0.38147684605757193
Jaccard: 0.37199147427191803
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09440219334182075
Normal: h_loss: 0.032569682467826426 macro F 0.2903432030523068 micro F 0.5174272367948256 micro P 0.6892150606253988 micro R 0.41418983700862894
Multi only: h_loss: 0.060511113286987175 macro F 0.2565563432809328 micro F 0.40289855072463765
Jaccard: 0.42971367134450006
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09482661396224557
Normal: h_loss: 0.032226120416478043 macro F 0.2780248941113316 micro F 0.5075659316904453 micro P 0.7132442284325637 micro R 0.3939597315436242
Multi only: h_loss: 0.06073142073827475 macro F 0.23871931438177776 micro F 0.38634677219886226
Jaccard: 0.4113512901765507
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.09497267616878856
Evaluating:
Evaluation results:
Evaluation Loss 0.0930507502970975
Normal: h_loss: 0.03200381555972321 macro F 0.2799639804328435 micro F 0.5120778900665517 micro P 0.7166264229044498 micro R 0.3983700862895494
Multi only: h_loss: 0.05962988348183688 macro F 0.24654605249894207 micro F 0.40294117647058825
Jaccard: 0.41156820582465703
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03198952328305562 macro F 0.28116215976374154 micro F 0.5052417302798982
Multi only: h_loss: 0.059907834101382486 macro F 0.23493584258828457 micro F 0.3948275862068965
Jaccard: 0.40194705484921084
Evaluating:
22500
22500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09580474130340484
Normal: h_loss: 0.03273944254025739 macro F 0.26071248187570306 micro F 0.500308451573103 micro P 0.7015570934256056 micro R 0.3887823585810163
Multi only: h_loss: 0.06038872025849407 macro F 0.23131494079037082 micro F 0.38586009459795867
Jaccard: 0.4045608872793122
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09680058243872085
Normal: h_loss: 0.03338614757808964 macro F 0.24854327107223467 micro F 0.522377703249682 micro P 0.6580710955710956 micro R 0.43307766059443914
Multi only: h_loss: 0.06178400078331538 macro F 0.21954516910441416 micro F 0.40415486307837584
Jaccard: 0.4461011770031693
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09501589934188567
Normal: h_loss: 0.03310725603052448 macro F 0.28715741074836604 micro F 0.5080184996095862 micro P 0.6800128638044701 micro R 0.4054650047938639
Multi only: h_loss: 0.059727797904631355 macro F 0.2587081435357594 micro F 0.4148681055155875
Jaccard: 0.4153595141089486
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09370520082586974
Normal: h_loss: 0.03266264631701481 macro F 0.2956851114132352 micro F 0.5406695844938328 micro P 0.6639676113360324 micro R 0.45599232981783316
Multi only: h_loss: 0.060829335161069226 macro F 0.26231793698825623 micro F 0.41735052754982416
Jaccard: 0.47124830239927606
saving best model ...
Training Loss for epoch 1: 0.09380947072903316
Evaluating:
Evaluation results:
Evaluation Loss 0.09323336553349652
Normal: h_loss: 0.03240800620836836 macro F 0.2906805905294117 micro F 0.5071305630686009 micro P 0.7065775950668037 micro R 0.39549376797698943
Multi only: h_loss: 0.059972583961617545 macro F 0.25720576976433424 micro F 0.400097943192948
Jaccard: 0.4098894673306176
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03196319987364764 macro F 0.2929461444249651 micro F 0.5066531234129
Multi only: h_loss: 0.059694487113841954 macro F 0.23976510534690054 micro F 0.3987967339922647
Jaccard: 0.4075056814691974
Evaluating:
23000
23000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09271262933855523
Normal: h_loss: 0.03247671861863804 macro F 0.3210087214746483 micro F 0.5142960768905278 micro P 0.6958939964011124 micro R 0.40786193672099713
Multi only: h_loss: 0.05874865367668657 macro F 0.28395923632542946 micro F 0.4291151284490961
Jaccard: 0.4151652331371666
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0916972277511456
Normal: h_loss: 0.03231504235917998 macro F 0.31398011442744905 micro F 0.5255474452554744 micro P 0.689612209936147 micro R 0.42454458293384467
Multi only: h_loss: 0.06063350631548027 macro F 0.26790648149321905 micro F 0.40442414041836977
Jaccard: 0.44204013882601506
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09030139551490829
Normal: h_loss: 0.03182192976783289 macro F 0.3393252662659477 micro F 0.5243187722796206 micro P 0.7088710994935468 micro R 0.41601150527325026
Multi only: h_loss: 0.05732889454616665 macro F 0.30439360253895004 micro F 0.44184938036224974
Jaccard: 0.42290817866304553
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09116231119905245
Normal: h_loss: 0.03235950333053095 macro F 0.33983513958184985 micro F 0.5506286484059273 micro P 0.6640942323314378 micro R 0.47027804410354745
Multi only: h_loss: 0.06073142073827475 macro F 0.29718308778177993 micro F 0.4247623463946209
Jaccard: 0.4894503546099293
saving best model ...
Training Loss for epoch 1: 0.0877494223351064
Evaluating:
Evaluation results:
Evaluation Loss 0.09139899770029804
Normal: h_loss: 0.031943186962426436 macro F 0.3080856194807597 micro F 0.5279818431583349 micro P 0.7001425629653096 micro R 0.42377756471716205
Multi only: h_loss: 0.05980123372172721 macro F 0.2636260528439323 micro F 0.41259918249579225
Jaccard: 0.44094612947034884
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031574929584879834 macro F 0.31345367301975885 micro F 0.527104277547802
Multi only: h_loss: 0.059822495306366276 macro F 0.2510400922351193 micro F 0.4084388185654008
Jaccard: 0.4358608193599904
Evaluating:
23500
23500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09171851557937075
Normal: h_loss: 0.031704714479725796 macro F 0.37351787880007964 micro F 0.5270139893873613 micro P 0.710107247318817 micro R 0.4189837008628955
Multi only: h_loss: 0.057475766180358365 macro F 0.32949929723026933 micro F 0.43666026871401153
Jaccard: 0.42867247623359017
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.0908756570130816
Normal: h_loss: 0.03212103084783031 macro F 0.3645539265069952 micro F 0.5318409425625921 micro P 0.6896867838044308 micro R 0.4327900287631831
Multi only: h_loss: 0.05759815920885147 macro F 0.32381951510274354 micro F 0.45113132726848615
Jaccard: 0.4391127206880946
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09031551492671559
Normal: h_loss: 0.03200381555972321 macro F 0.330661382142201 micro F 0.5156001468249113 micro P 0.7123056118999324 micro R 0.4040268456375839
Multi only: h_loss: 0.058871046705179675 macro F 0.28465876946656304 micro F 0.418660865361373
Jaccard: 0.41539723856948896
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09071811518959462
Normal: h_loss: 0.031631960162969666 macro F 0.3564031007470373 micro F 0.5314333612740989 micro P 0.7075892857142857 micro R 0.425503355704698
Multi only: h_loss: 0.06065798492117889 macro F 0.28515499135288835 micro F 0.3899556868537666
Jaccard: 0.44884563150746976
saving best model ...
Training Loss for epoch 1: 0.08266172405379883
Evaluating:
Evaluation results:
Evaluation Loss 0.09067515224798495
Normal: h_loss: 0.03225845566836966 macro F 0.35087685562344945 micro F 0.54640522875817 micro P 0.6709002093510119 micro R 0.46088207094918504
Multi only: h_loss: 0.05980123372172721 macro F 0.28691757645095023 micro F 0.42422814046665097
Jaccard: 0.4766108344650674
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.0322264339677275 macro F 0.34721523859168457 micro F 0.5408345053914675
Multi only: h_loss: 0.06033452807646356 macro F 0.26044984237870933 micro F 0.415702479338843
Jaccard: 0.4678152447638355
Evaluating:
24000
24000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09169996271964757
Normal: h_loss: 0.032379712862963206 macro F 0.397563448319752 micro F 0.5538787102522693 micro P 0.6606881891855985 micro R 0.47679769894534996
Multi only: h_loss: 0.057622637814550084 macro F 0.35306513928593786 micro F 0.46790235081374326
Jaccard: 0.4821110608118307
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09202729653771131
Normal: h_loss: 0.03296578930349867 macro F 0.40410803446592053 micro F 0.5502867225408029 micro P 0.6475473656890735 micro R 0.4784276126558006
Multi only: h_loss: 0.05610496426123568 macro F 0.3627221108324089 micro F 0.4837837837837837
Jaccard: 0.47617134449977416
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09441569876709963
Normal: h_loss: 0.0319714803078316 macro F 0.3556911353122871 micro F 0.5440922190201729 micro P 0.6820809248554913 micro R 0.45254074784276127
Multi only: h_loss: 0.05999706256731616 macro F 0.2936805005818685 micro F 0.42070432521862444
Jaccard: 0.47221593481213237
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08954661792323783
Normal: h_loss: 0.03199977365323676 macro F 0.3953575243007043 micro F 0.547780887644942 micro P 0.6775469831849654 micro R 0.4597315436241611
Multi only: h_loss: 0.057696073631645944 macro F 0.3389469902637217 micro F 0.45427182218106044
Jaccard: 0.46642900256526354
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.07699940631786982
Evaluating:
Evaluation results:
Evaluation Loss 0.09007941953578164
Normal: h_loss: 0.03195127077539934 macro F 0.41148021127322976 micro F 0.5603692786830543 micro P 0.6671963978281023 micro R 0.48302972195589644
Multi only: h_loss: 0.0571330657005777 macro F 0.3614258852595939 micro F 0.4714673913043479
Jaccard: 0.48916742115587775
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031140593329648057 macro F 0.41604003996566713 micro F 0.5673006583760059
Multi only: h_loss: 0.056366274108209595 macro F 0.33563936341911443 micro F 0.47848401105408606
Jaccard: 0.49350469872857955
Evaluating:
24500
24500
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09760427601119413
Normal: h_loss: 0.033018334087822544 macro F 0.42269947143476233 micro F 0.5552833578311285 micro P 0.6423982869379015 micro R 0.48897411313518696
Multi only: h_loss: 0.05823460295701557 macro F 0.37124603828945163 micro F 0.46885465505693236
Jaccard: 0.4875754489210808
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09553129389382908
Normal: h_loss: 0.03341039901700834 macro F 0.3998742644369702 micro F 0.5446231820185103 micro P 0.6401191401191402 micro R 0.47392138063279005
Multi only: h_loss: 0.05894448252227553 macro F 0.35110893952526684 micro F 0.4561878952122854
Jaccard: 0.47794062169911
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09358253055949889
Normal: h_loss: 0.03269902347539287 macro F 0.4148007591692374 micro F 0.5474379055717161 micro P 0.6571313456889605 micro R 0.4691275167785235
Multi only: h_loss: 0.05742680896896113 macro F 0.35780004504917795 micro F 0.46413887619917765
Jaccard: 0.4732854232684475
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09378810069912048
Normal: h_loss: 0.03320426178619931 macro F 0.4190930035871631 micro F 0.5426710460390803 micro P 0.6470197796362671 micro R 0.4673058485139022
Multi only: h_loss: 0.058797610888083815 macro F 0.3693442823292445 micro F 0.4572977858111162
Jaccard: 0.47004866455409755
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.0701849896031983
Evaluating:
Evaluation results:
Evaluation Loss 0.09450374046849039
Normal: h_loss: 0.03245650908620578 macro F 0.39112581018977705 micro F 0.5438018406999204 micro P 0.6673173452314557 micro R 0.4588686481303931
Multi only: h_loss: 0.05970331929893273 macro F 0.32380199655563796 micro F 0.43685061186792884
Jaccard: 0.46920552286102346
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03147621679959989 macro F 0.3945843832452545 micro F 0.5541989001770902
Multi only: h_loss: 0.05777436422597713 macro F 0.32624661664537663 micro F 0.4575320512820513
Jaccard: 0.4740034395921627
Evaluating:
25000
25000
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09581456061717394
Normal: h_loss: 0.03299004074241738 macro F 0.4092804734287405 micro F 0.5462026020237962 micro P 0.6500794070937004 micro R 0.47094918504314476
Multi only: h_loss: 0.05806325271712523 macro F 0.36369592049318983 micro F 0.45992714025500914
Jaccard: 0.47508676625924284
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09394074534266283
Normal: h_loss: 0.03269498156890642 macro F 0.4288928008546288 micro F 0.5574703211335412 micro P 0.6491272773601733 micro R 0.4884947267497603
Multi only: h_loss: 0.05615392147263292 macro F 0.39412025308412 micro F 0.48863129736959426
Jaccard: 0.4848706051003476
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09650098163282078
Normal: h_loss: 0.032517137683502555 macro F 0.4132256922579325 micro F 0.5636491837066768 micro P 0.6489321843387036 micro R 0.49817833173537873
Multi only: h_loss: 0.05811220992852247 macro F 0.3730207119549177 micro F 0.4684281236005374
Jaccard: 0.5014165534932853
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09536967127537123
Normal: h_loss: 0.03310321412403803 macro F 0.40148209941802104 micro F 0.5551330798479087 micro P 0.6403508771929824 micro R 0.4899328859060403
Multi only: h_loss: 0.05718202291197493 macro F 0.3555483793539346 micro F 0.4795008912655971
Jaccard: 0.4917138222423424
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.0699648666715622
Evaluating:
Evaluation results:
Evaluation Loss 0.09470435982715927
Normal: h_loss: 0.032832406389445776 macro F 0.38583351929116066 micro F 0.5453629596462752 micro P 0.6551028640580879 micro R 0.4671140939597315
Multi only: h_loss: 0.0587731322823852 macro F 0.33508504752448226 micro F 0.44613610149942334
Jaccard: 0.47722762939489993
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03225275737713549 macro F 0.4124010994650823 micro F 0.5638515618047522
Multi only: h_loss: 0.058926437958696024 macro F 0.32483281759914545 micro F 0.4569406213134094
Jaccard: 0.49912474663718454
