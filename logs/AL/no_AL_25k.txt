
loading file
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.15208362706002201
Normal: h_loss: 0.042157084653689454 macro F 0.0 micro F 0.0 micro P 0.0 micro R 0.0
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.151341453417225
Normal: h_loss: 0.042153042747203 macro F 9.801519235481501e-05 micro F 0.0007665037846124365 micro P 0.5714285714285714 micro R 0.0003835091083413231
Multi only: h_loss: 0.07473318319788505 macro F 0.0 micro F 0.0
Jaccard: 0.0004526935264825713
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1465740978124186
Normal: h_loss: 0.041130440406130765 macro F 0.011608051155355325 micro F 0.11157674174960712 micro P 0.6240234375 micro R 0.061265580057526366
Multi only: h_loss: 0.07512484088906296 macro F 0.006510416666666666 micro F 0.022300095571838163
Jaccard: 0.0703183944469594
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.13544080122097849
Normal: h_loss: 0.04049181918127142 macro F 0.04064380676702663 micro F 0.2609914429035114 micro P 0.5658989123480487 micro R 0.16960690316395013
Multi only: h_loss: 0.07446391853520024 macro F 0.03875136433093211 micro F 0.14454443194600675
Jaccard: 0.1852459634827222
saving best model ...
Training Loss for epoch 1: 0.1564231715965271
Evaluating:
Evaluation results:
Evaluation Loss 0.13395984651627935
Normal: h_loss: 0.03992595227316821 macro F 0.05476404017647415 micro F 0.3702664796633941 micro P 0.5525114155251142 micro R 0.27842761265580057
Multi only: h_loss: 0.07527171252325468 macro F 0.04927347064130372 micro F 0.18543046357615894
Jaccard: 0.3083880338011166
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.039557503487851746 macro F 0.054424534971333816 micro F 0.37326660410801793
Multi only: h_loss: 0.0742020822665984 macro F 0.05075551564099505 micro F 0.19750807568066453
Jaccard: 0.309716847859468
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.12771409999574132
Normal: h_loss: 0.038094968634805666 macro F 0.06390701966211887 micro F 0.3095743901545674 micro P 0.6560074511021422 micro R 0.20258868648130393
Multi only: h_loss: 0.07142857142857142 macro F 0.057726518950712395 micro F 0.1614942528735632
Jaccard: 0.22260261053266936
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.1234336614325447
Normal: h_loss: 0.03671263661643924 macro F 0.1123157578458405 micro F 0.4156963653908009 micro P 0.6316715542521995 micro R 0.30977948226270374
Multi only: h_loss: 0.06883383922451777 macro F 0.09993196360539126 micro F 0.25687103594080335
Jaccard: 0.3254866455409688
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.11524415767978036
Normal: h_loss: 0.034889736791049605 macro F 0.1376022897348062 micro F 0.42872270019854397 micro P 0.6920940170940171 micro R 0.31054650047938637
Multi only: h_loss: 0.06655732889454617 macro F 0.12019417930405166 micro F 0.27241102488627245
Jaccard: 0.3345216538403501
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10927519010318357
Normal: h_loss: 0.0344895880488909 macro F 0.14629272159310397 micro F 0.4307918084183844 micro P 0.7079587809690857 micro R 0.3095877277085331
Multi only: h_loss: 0.06459904043865661 macro F 0.1366016644112422 micro F 0.3038776048536006
Jaccard: 0.32648634374528446
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.12190882269382476
Evaluating:
Evaluation results:
Evaluation Loss 0.10824993354950949
Normal: h_loss: 0.0343481213218651 macro F 0.14611534959918052 micro F 0.44717668488160295 micro P 0.6954674220963173 micro R 0.3295302013422819
Multi only: h_loss: 0.0643297757759718 macro F 0.13759667744489607 micro F 0.31846473029045647
Jaccard: 0.3484136864342841
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03381900023691069 macro F 0.14767867988743302 micro F 0.4478349629311271
Multi only: h_loss: 0.06259600614439324 macro F 0.14274551585741 micro F 0.3316628701594533
Jaccard: 0.34520299735888454
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10516664201301813
Normal: h_loss: 0.03484527581969864 macro F 0.16313763246712862 micro F 0.4857125812802004 micro P 0.6428233064898152 micro R 0.3903163950143816
Multi only: h_loss: 0.06530892000391658 macro F 0.15672135706519233 micro F 0.34447174447174445
Jaccard: 0.4161385242191038
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10271457658938123
Normal: h_loss: 0.03329318372890125 macro F 0.16816698861663762 micro F 0.451853330671458 micro P 0.738525125081575 micro R 0.32550335570469796
Multi only: h_loss: 0.06388916087339665 macro F 0.1470376849711857 micro F 0.3160377358490566
Jaccard: 0.3459898898445753
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09914089912011538
Normal: h_loss: 0.03327701610295544 macro F 0.19876532079971304 micro F 0.4641718190693133 micro P 0.7225937183383991 micro R 0.34189837008628954
Multi only: h_loss: 0.06229805150298639 macro F 0.18626389274158847 micro F 0.3505996427660117
Jaccard: 0.3590406669684623
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09680103103517676
Normal: h_loss: 0.03282028066998642 macro F 0.21449020345693734 micro F 0.48879375472173253 micro P 0.7117711771177118 micro R 0.37219558964525407
Multi only: h_loss: 0.06197982962890434 macro F 0.19026061904632158 micro F 0.35963581183611526
Jaccard: 0.39284366983552155
patience 3 not best model , ignoring ...
Training Loss for epoch 1: 0.10170017779350281
Evaluating:
Evaluation results:
Evaluation Loss 0.09768115137499239
Normal: h_loss: 0.033260848477009636 macro F 0.2382367247648503 micro F 0.498934421238507 micro P 0.6836309027198398 micro R 0.3928092042186002
Multi only: h_loss: 0.06349750318221874 macro F 0.21206214714792113 micro F 0.3569657907783837
Jaccard: 0.4141711936019317
patience 4 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.0326147042564953 macro F 0.2362539253163858 micro F 0.5012077294685989
Multi only: h_loss: 0.061614610001706774 macro F 0.20743354672707484 micro F 0.3721739130434783
Jaccard: 0.4098550457588601
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10224171180148209
Normal: h_loss: 0.03363270387376318 macro F 0.16446672689004777 micro F 0.4618767380197892 micro P 0.7095171865686469 micro R 0.3423777564717162
Multi only: h_loss: 0.06401155390188974 macro F 0.15430153579054373 micro F 0.3227143227143227
Jaccard: 0.36389957748604196
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10080644565572454
Normal: h_loss: 0.03288899308025609 macro F 0.21580777205227383 micro F 0.4856835851084002 micro P 0.7126692635874606 micro R 0.36836049856184083
Multi only: h_loss: 0.061930872417507096 macro F 0.19978681477122742 micro F 0.3665498247371056
Jaccard: 0.3849950958201301
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09896501728437669
Normal: h_loss: 0.03313150746944319 macro F 0.22719516039897344 micro F 0.49479198767334354 micro P 0.6926660914581536 micro R 0.38485139022051773
Multi only: h_loss: 0.061808479389014 macro F 0.2070668059277634 micro F 0.3751546646869587
Jaccard: 0.40131658367285367
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09639623242098587
Normal: h_loss: 0.03313554937592964 macro F 0.23061377527322052 micro F 0.5067982192275298 micro P 0.6802325581395349 micro R 0.40383509108341326
Multi only: h_loss: 0.06100068540095956 macro F 0.2182929192602733 micro F 0.3916015625
Jaccard: 0.4181982797645997
saving best model ...
Training Loss for epoch 1: 0.09908818386554719
Evaluating:
Evaluation results:
Evaluation Loss 0.0968822367883959
Normal: h_loss: 0.03322851322511802 macro F 0.21589971973919225 micro F 0.44947431862318354 micro P 0.7452809238285587 micro R 0.3217641418983701
Multi only: h_loss: 0.06095172818956232 macro F 0.19623444070691373 micro F 0.3605546995377504
Jaccard: 0.33162441527086167
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03297007028350312 macro F 0.21367787314640513 micro F 0.44395116537180906
Multi only: h_loss: 0.06110257723160949 macro F 0.18774619598978087 micro F 0.35727109515260325
Jaccard: 0.32256925250291746
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09505135076935878
Normal: h_loss: 0.03273944254025739 macro F 0.2767424061240104 micro F 0.5007396449704142 micro P 0.7010700724887815 micro R 0.3894534995206136
Multi only: h_loss: 0.05889552531087829 macro F 0.26435383143460894 micro F 0.4196816208393632
Jaccard: 0.3989908706805496
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09500495740707884
Normal: h_loss: 0.03250097005755675 macro F 0.24337689261013934 micro F 0.5119864052922255 micro P 0.6975359682487183 micro R 0.40441035474592524
Multi only: h_loss: 0.061612650543425046 macro F 0.21049075932738506 micro F 0.38534798534798537
Jaccard: 0.4225233891655351
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09312215217736146
Normal: h_loss: 0.03201998318566902 macro F 0.2584304761020297 micro F 0.48431193854966803 micro P 0.754257907542579 micro R 0.3566634707574305
Multi only: h_loss: 0.060829335161069226 macro F 0.2253769478166477 micro F 0.36396211927309957
Jaccard: 0.3750414969065943
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09243176799754904
Normal: h_loss: 0.03228674901377482 macro F 0.2827670232680849 micro F 0.5162306201550387 micro P 0.7007563301545544 micro R 0.40862895493767976
Multi only: h_loss: 0.05994810535591893 macro F 0.2450109314710379 micro F 0.4016613730759833
Jaccard: 0.42592802172928956
saving best model ...
Training Loss for epoch 1: 0.09100213932991028
Evaluating:
Evaluation results:
Evaluation Loss 0.09384014550891598
Normal: h_loss: 0.032682855849447065 macro F 0.22940997445253392 micro F 0.5064697265625 micro P 0.6968424588511924 micro R 0.3977948226270374
Multi only: h_loss: 0.06261627337706845 macro F 0.2005285909297351 micro F 0.36145781328007986
Jaccard: 0.4209578240531163
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03214088288715154 macro F 0.23832677766989716 micro F 0.5096385542168674
Multi only: h_loss: 0.06187062638675542 macro F 0.1894637932991042 micro F 0.36291739894551844
Jaccard: 0.4210122228364351
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09210933768517186
Normal: h_loss: 0.03232312617215288 macro F 0.31670632135390747 micro F 0.5071186440677966 micro P 0.7099223468507334 micro R 0.39443911792905084
Multi only: h_loss: 0.058014295505727995 macro F 0.28182255069774803 micro F 0.41940225379715823
Jaccard: 0.40655085257280843
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09243874175279267
Normal: h_loss: 0.03231908426566643 macro F 0.3030483336094834 micro F 0.5454235361000568 micro P 0.6699720670391062 micro R 0.4599232981783317
Multi only: h_loss: 0.05874865367668657 macro F 0.2750155790868017 micro F 0.4444444444444445
Jaccard: 0.4702618077561495
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08998697146861032
Normal: h_loss: 0.031563247752699995 macro F 0.32400086308845216 micro F 0.5309628205898252 micro P 0.710725196977006 micro R 0.42377756471716205
Multi only: h_loss: 0.05899343973367277 macro F 0.2893622937890122 micro F 0.42344497607655507
Jaccard: 0.43653614003319785
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09090402821857177
Normal: h_loss: 0.03195127077539934 macro F 0.3387207935508481 micro F 0.5491873396065012 micro P 0.6776917663617171 micro R 0.46164908916586767
Multi only: h_loss: 0.058479389014001765 macro F 0.3060134687409869 micro F 0.4499194105457057
Jaccard: 0.47205371963180964
saving best model ...
Training Loss for epoch 1: 0.08424158444166184
Evaluating:
Evaluation results:
Evaluation Loss 0.09082497061040942
Normal: h_loss: 0.03186234883269741 macro F 0.3255065989627745 micro F 0.5229652042360061 micro P 0.7089417555373256 micro R 0.4142857142857143
Multi only: h_loss: 0.05992362675022031 macro F 0.2860379479430176 micro F 0.4081237911025145
Jaccard: 0.42965142598460865
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031252467819632 macro F 0.3369272492690734 micro F 0.5249574872461739
Multi only: h_loss: 0.058073050008533876 macro F 0.29654819890804646 micro F 0.42109740535942153
Jaccard: 0.42586450463730724
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09232261274035239
Normal: h_loss: 0.03188255836512966 macro F 0.35165697028566006 micro F 0.5260185073909385 micro P 0.70460399227302 micro R 0.4196548418024928
Multi only: h_loss: 0.05896896112797415 macro F 0.2962214298168414 micro F 0.4229940119760479
Jaccard: 0.43144522408329594
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09140990656282418
Normal: h_loss: 0.03195531268188579 macro F 0.37154536906460606 micro F 0.5519156653819995 micro P 0.6749376212919324 micro R 0.46682646212847556
Multi only: h_loss: 0.057304415940468034 macro F 0.33019803450790813 micro F 0.46393405083581407
Jaccard: 0.4691338463859971
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.08960644672626875
Normal: h_loss: 0.03135306861540452 macro F 0.3392355274753728 micro F 0.53553679420394 micro P 0.713123903683623 micro R 0.42876318312559925
Multi only: h_loss: 0.05923822579065897 macro F 0.28724408563668735 micro F 0.4151764137264379
Jaccard: 0.4478082088426139
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09125310730150298
Normal: h_loss: 0.031825971674319345 macro F 0.342407325147071 micro F 0.5465852815847058 micro P 0.6842560553633218 micro R 0.45503355704697984
Multi only: h_loss: 0.05884656809948105 macro F 0.2967379655645589 micro F 0.4404096834264432
Jaccard: 0.468798098687189
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.07800868487119675
Evaluating:
Evaluation results:
Evaluation Loss 0.0895660731329293
Normal: h_loss: 0.03190276789756192 macro F 0.3661178526950201 micro F 0.5503845058387924 micro P 0.6780350877192982 micro R 0.46318312559923297
Multi only: h_loss: 0.057304415940468034 macro F 0.32048316929786125 micro F 0.46319651456088057
Jaccard: 0.47112381167949335
saving best model ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.031495959356655875 macro F 0.3708116282316331 micro F 0.5517887244802396
Multi only: h_loss: 0.0570063150708312 macro F 0.30980931145402085 micro F 0.4595469255663431
Jaccard: 0.468966893925435
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09302149523486375
Normal: h_loss: 0.03234333570458514 macro F 0.3867475482880642 micro F 0.5475005654829224 micro P 0.66735594154949 micro R 0.4641418983700863
Multi only: h_loss: 0.057084108489180455 macro F 0.33704257869092696 micro F 0.4648921523634695
Jaccard: 0.47092764448468427
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09393554923557598
Normal: h_loss: 0.03259797581323159 macro F 0.39463019536824046 micro F 0.5629436947921748 micro P 0.6473887573226973 micro R 0.49798657718120803
Multi only: h_loss: 0.057549201997454225 macro F 0.34738415362866215 micro F 0.47510605045769144
Jaccard: 0.5032329862682968
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09258047773449224
Normal: h_loss: 0.03286474164133739 macro F 0.4086954370056817 micro F 0.5565312244341423 micro P 0.6454142947501581 micro R 0.4891658676893576
Multi only: h_loss: 0.057304415940468034 macro F 0.36235310957126926 micro F 0.4731037587215845
Jaccard: 0.4909310396861326
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09300715712372562
Normal: h_loss: 0.032614143439177394 macro F 0.36115871054217125 micro F 0.511945805358979 micro P 0.6934294609208586 micro R 0.40575263662511984
Multi only: h_loss: 0.05843043180260452 macro F 0.30102837922970466 micro F 0.4279894560268392
Jaccard: 0.4140995171269053
patience 2 not best model , ignoring ...
Training Loss for epoch 1: 0.07102886174917221
Evaluating:
Evaluation results:
Evaluation Loss 0.0923561333291209
Normal: h_loss: 0.03245246717971933 macro F 0.3805407877043117 micro F 0.5364586340280584 micro P 0.6742127412567116 micro R 0.4454458293384468
Multi only: h_loss: 0.05821012435131695 macro F 0.3273760955247405 micro F 0.44283036551077787
Jaccard: 0.4563716613852426
patience 3 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03191055305483166 macro F 0.38881779839176456 micro F 0.5363801510660675
Multi only: h_loss: 0.05713432326335552 macro F 0.31900233769413805 micro F 0.4496506370735717
Jaccard: 0.4492506602788527
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09504114197471823
Normal: h_loss: 0.032771777792149 macro F 0.3907380953692668 micro F 0.5509525919361985 micro P 0.6522423288749016 micro R 0.4768935762224353
Multi only: h_loss: 0.05914031136786449 macro F 0.3264783960017709 micro F 0.4524025385312784
Jaccard: 0.4855326693828283
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09318556526280355
Normal: h_loss: 0.0325130957770161 macro F 0.40768037539939517 micro F 0.5483436271757439 micro P 0.6616531165311653 micro R 0.46816874400767017
Multi only: h_loss: 0.05745128757465975 macro F 0.34888116117662155 micro F 0.4618206833295116
Jaccard: 0.474794401690056
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09473430221304197
Normal: h_loss: 0.033256806570523184 macro F 0.4242455791562078 micro F 0.5363984674329503 micro P 0.6504509428805685 micro R 0.4563758389261745
Multi only: h_loss: 0.05659453637520807 macro F 0.39099354035602857 micro F 0.4757369614512471
Jaccard: 0.45834653689452287
patience 2 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09434708454736815
Normal: h_loss: 0.03255755674836707 macro F 0.39705001133398415 micro F 0.5567600286138777 micro P 0.6533643290714194 micro R 0.4850431447746884
Multi only: h_loss: 0.05710858709487908 macro F 0.3590745507171219 micro F 0.478659217877095
Jaccard: 0.4887599969820436
saving best model ...
Training Loss for epoch 1: 0.06771649349212647
Evaluating:
Evaluation results:
Evaluation Loss 0.09494741620211755
Normal: h_loss: 0.032783903511608356 macro F 0.3918854313977726 micro F 0.5553423606161944 micro P 0.6484445013442581 micro R 0.4856184084372004
Multi only: h_loss: 0.057157544306276314 macro F 0.3571589746129301 micro F 0.48030269307812157
Jaccard: 0.48487437754640134
patience 1 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03179867856484772 macro F 0.3962137395372825 micro F 0.5642135642135642
Multi only: h_loss: 0.0564089435057177 macro F 0.3372102962639848 micro F 0.4815686274509804
Jaccard: 0.4891130765923473
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.09902294068489748
Normal: h_loss: 0.03347911142727802 macro F 0.42094053398521003 micro F 0.5460623664163972 micro P 0.6373288985544326 micro R 0.4776605944391179
Multi only: h_loss: 0.05732889454616665 macro F 0.37228707286245866 micro F 0.47037539574853005
Jaccard: 0.47611098536290974
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10051556858084439
Normal: h_loss: 0.033188094160253506 macro F 0.41531026532562426 micro F 0.5516789516789516 micro P 0.640710209258085 micro R 0.4843720038350911
Multi only: h_loss: 0.05737785175756389 macro F 0.3746884199428578 micro F 0.4802660753880266
Jaccard: 0.4836634223630606
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10558923918394655
Normal: h_loss: 0.03421878031429865 macro F 0.4281070419102668 micro F 0.5636082474226805 micro P 0.6094760312151617 micro R 0.5241610738255034
Multi only: h_loss: 0.05818564574561833 macro F 0.3911303131248644 micro F 0.48914678701912734
Jaccard: 0.5182095971027616
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.09801626321301032
Normal: h_loss: 0.03305471124620061 macro F 0.3826185943458367 micro F 0.5402518551832697 micro P 0.6530307148681707 micro R 0.4606903163950144
Multi only: h_loss: 0.05869969646528934 macro F 0.32185673310308277 micro F 0.448735632183908
Jaccard: 0.4684944167798404
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.06017316491603851
Evaluating:
Evaluation results:
Evaluation Loss 0.10037524879970341
Normal: h_loss: 0.033422524736467694 macro F 0.4028346140663642 micro F 0.5552627332867208 micro P 0.6323655518804361 micro R 0.4949185043144775
Multi only: h_loss: 0.05867521785959072 macro F 0.3511315753641417 micro F 0.46697798532354895
Jaccard: 0.4982024294552591
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03262786596119929 macro F 0.4156702699158115 micro F 0.5616268788682582
Multi only: h_loss: 0.05739033964840416 macro F 0.36480022820476893 micro F 0.47604207245812236
Jaccard: 0.5004913703089493
Evaluating:
Training on epoch=1 -------------------------
Evaluating:
Evaluation results:
Evaluation Loss 0.10431907678781024
Normal: h_loss: 0.034340037508892196 macro F 0.4275191428294801 micro F 0.5400606323083585 micro P 0.6202437204675454 micro R 0.47823585810162994
Multi only: h_loss: 0.058724175070987955 macro F 0.38963330501220467 micro F 0.4728631070094485
Jaccard: 0.47666930737890506
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10338678395973444
Normal: h_loss: 0.033762044881329625 macro F 0.42498945856176207 micro F 0.5403620756066693 micro P 0.6341211416763528 micro R 0.4707574304889741
Multi only: h_loss: 0.058087731322823855 macro F 0.377272372107028 micro F 0.47043070743137694
Jaccard: 0.4700411196619895
patience 1 not best model , ignoring ...
Evaluating:
Evaluation results:
Evaluation Loss 0.11172774300525608
Normal: h_loss: 0.03400860117700317 macro F 0.4049581498488447 micro F 0.5522562792677734 micro P 0.6205453240851471 micro R 0.4975071907957814
Multi only: h_loss: 0.058650739253892095 macro F 0.3678350072539147 micro F 0.47178130511463845
Jaccard: 0.4963350686585183
saving best model ...
Evaluating:
Evaluation results:
Evaluation Loss 0.10716826260433902
Normal: h_loss: 0.03355994955700705 macro F 0.41769562850989306 micro F 0.5526159814645186 micro P 0.6308279001107148 micro R 0.49165867689357623
Multi only: h_loss: 0.058797610888083815 macro F 0.3772348274575522 micro F 0.4683488269145639
Jaccard: 0.4934151954127059
patience 1 not best model , ignoring ...
Training Loss for epoch 1: 0.05405656282782555
Evaluating:
Evaluation results:
Evaluation Loss 0.10545676040697982
Normal: h_loss: 0.03391159542132833 macro F 0.4266420206070842 micro F 0.5489247311827956 micro P 0.6248470012239902 micro R 0.4894534995206136
Multi only: h_loss: 0.057867423871536275 macro F 0.38962323744292815 micro F 0.4829396325459317
Jaccard: 0.4833748302399281
patience 2 not best model , ignoring ...
Testing:
NOTE, this is on the test set
Normal: h_loss: 0.03350311932401485 macro F 0.4126471053608241 micro F 0.5553323434361079
Multi only: h_loss: 0.05896910735620413 macro F 0.3476229655539882 micro F 0.4618380062305296
Jaccard: 0.5006756341748054
